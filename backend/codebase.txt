Folder Structure
--------------------------------------------------
vizier/
    .gitignore
    database.py
    dummyapi.py
    main.py
    README.md
    requirements.txt
    test.py
        processes/
            main.py
            connectors/
                router_0.py
                router_04.py
            query/
                refiner.py
            report/
                refiner.py
                writer.py
            search/
                agents.py
                twitter.py
                web.py
            sourcing/
                agent.py
                director.py
            writer/
                generator.py
        routers/
            auth.py
            drafts.py
            openrouter.py
            queries.py
            user.py


File Contents
--------------------------------------------------


.\.gitignore
File type: 
*.env
*__pycache__/
*.venv/

--------------------------------------------------
File End
--------------------------------------------------


.\database.py
File type: .py
import os
from dotenv import load_dotenv
from databases import Database
from fastapi import FastAPI

load_dotenv(override=True)

DATABASE_URL = os.getenv("DATABASE_URL")
database = Database(DATABASE_URL)

async def init_db():
    """Initialize database tables and columns"""
    await database.execute("""
        CREATE TABLE IF NOT EXISTS queries (
            query_id UUID PRIMARY KEY,
            user_id VARCHAR NOT NULL,
            query_text TEXT NOT NULL,
            refined_query TEXT,
            status VARCHAR NOT NULL,
            web_sources JSONB DEFAULT '[]'::jsonb,
            twitter_sources JSONB DEFAULT '[]'::jsonb,
            final_sources JSONB DEFAULT '[]'::jsonb,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

def register_lifespan_events(app: FastAPI):
    @app.on_event("startup")
    async def startup():
        await database.connect()
        await init_db()

    @app.on_event("shutdown")
    async def shutdown():
        await database.disconnect()


--------------------------------------------------
File End
--------------------------------------------------


.\dummyapi.py
File type: .py
from fastapi import FastAPI, APIRouter, HTTPException, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from typing import List
from uuid import uuid4

app = FastAPI()

# Allow CORS for local frontend testing
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Dummy Auth Dependency
def get_current_user():
    return "dummy-user-id"

# Routers
auth_router = APIRouter(prefix="/auth", tags=["auth"])
user_router = APIRouter(prefix="/user", tags=["user"])
query_router = APIRouter(prefix="/queries", tags=["queries"])
draft_router = APIRouter(prefix="/drafts", tags=["drafts"])
publish_router = APIRouter(prefix="/publish", tags=["publish"])

# ===== AUTH ROUTES =====
@auth_router.get("/callback")
def auth_callback(code: str):
    return {"access_token": "dummy.jwt.token.from.google"}

@auth_router.get("/me")
def auth_me(user_id: str = Depends(get_current_user)):
    return {"user_id": user_id}

# ===== USER ROUTES =====
@user_router.get("/me")
def get_user_profile(user_id: str = Depends(get_current_user)):
    return {
        "name": "user full name",
        "email": "user@example.com",
        "archetype": "Researcher",
        "user_experience": 7
    }

@user_router.post("/profile")
def update_user_profile(data: dict, user_id: str = Depends(get_current_user)):
    return {"message": "Profile updated", "data": data}

# ===== QUERY ROUTES =====
@query_router.get("/")
def get_queries(user_id: str = Depends(get_current_user)):
    return [{"query_id": str(uuid4()), "initial_query": "What's happening in AI?"}]

@query_router.post("/")
def submit_query(data: dict, user_id: str = Depends(get_current_user)):
    return {"query_id": str(uuid4())}

# ===== DRAFT ROUTES =====
@draft_router.post("/generate")
def generate_draft(data: dict, user_id: str = Depends(get_current_user)):
    return {"draft_id": str(uuid4()), "status": "pending"}

@draft_router.get("/{draft_id}")
def get_draft_by_id(draft_id: str, user_id: str = Depends(get_current_user)):
    return {
        "draft_id": draft_id,
        "content": "AI is evolving rapidly.",
        "status": "accepted",
        "sources": ["source1", "source2"],
        "generated_at": "2024-04-12T15:30:00Z"
    }

@draft_router.post("/accept")
def accept_draft(data: dict, user_id: str = Depends(get_current_user)):
    return {"message": "Draft accepted"}

@draft_router.post("/reject")
def reject_draft(data: dict, user_id: str = Depends(get_current_user)):
    return {"message": "Draft rejected"}

# ===== PUBLISH/SHARE ROUTES =====
@publish_router.post("/{draft_id}")
def publish_draft(draft_id: str, user_id: str = Depends(get_current_user)):
    return {
        "public_id": "abc123xyz",
        "share_url": f"http://localhost:8000/share/abc123xyz"
    }

@app.get("/share/{public_id}")
def get_public_draft(public_id: str):
    return {"content": "This is a public draft.", "sources": ["source1", "source2"]}

# Register routers
app.include_router(auth_router)
app.include_router(user_router)
app.include_router(query_router)
app.include_router(draft_router)
app.include_router(publish_router)


--------------------------------------------------
File End
--------------------------------------------------


.\main.py
File type: .py
from fastapi import FastAPI
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from sse_starlette.sse import EventSourceResponse
from routers import auth, user, queries, drafts
from database import register_lifespan_events

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173"
    ],
    allow_credentials=True,  # required for cookies/auth headers
    allow_methods=["*"],
    allow_headers=["*"],     # includes Authorization
)

# Global process state tracker
process_states = {}

register_lifespan_events(app)
app.include_router(auth.router)
app.include_router(user.router)
app.include_router(queries.router)
app.include_router(drafts.router)


--------------------------------------------------
File End
--------------------------------------------------


.\README.md
File type: .md
# Vizier

### From Noise to Signal: Because 'Insight' Isn't Just Another Token

## ðŸš€ Inspiration

Every serious researcher â€” from grad students grinding out their lit reviews to tenured professors trying to track five subfields at once â€” knows the pain:

- New research moves **too fast**
- It's **scattered across a dozen platforms**
- And if you blink, you've missed a whole trend

Between arXiv, Twitter threads, conference keynotes, and shadow releases on GitHub, **there's no single feed that captures the full signal**.

We weren't inspired by another one-shot summarizer. We were inspired by how actual researchers work:
- Constant pivots between sources
- Zooming in and out across timescales
- Evaluating claims, not just regurgitating them
- Prioritizing trust, depth, and novelty

So we built Vizier â€” not another autocomplete wrapper, but a system that actually understands your goals and assembles a live research team around them.

Because in 2025, **Insight isn't just another token.** 

## ðŸ§  What Vizier Actually Does

Vizier is a **modular, agentic research engine** â€” your personal research ops team, not just a chatbot. Whether you're building a newsletter, writing a paper, pitching an idea, or just staying on the bleeding edge, Vizier gives you:

- ðŸ” Precision-curated content from **credible, multi-platform sources**
- ðŸ§± Structured, editable reports tailored to your research priorities
- ðŸ§  Control over what gets emphasized, where deeper sourcing is needed, and how frequent updates should be

### Built for:

- ðŸ§‘â€ðŸ”¬ **Researchers and technical professionals** who need rigorous updates on specialized domains
- ðŸŽ“ **Students and professors** tracking rapid fields like GenAI, climate science, or synthetic bio
- ðŸ“£ **Content creators and analysts** writing newsletters, reports, or breakdowns on bleeding-edge developments

Once you've locked in a great output, you can:
- ðŸ“… **Schedule that research plan to auto-run** daily, weekly, or monthly
- ðŸ” Revisit past reports, tweak scopes, swap source weights, or layer in new domains

## ðŸ” What Makes Vizier *Agentic*?

This isn't just "use LangChain and call it a day." Vizier's agents **think for themselves**.

### ðŸ§­ Router v0_4
Analyzes your refined query and decides:
- How many agents to spawn
- Which domains get which budget
- Which model contexts are needed

### ðŸ”Ž Web and Twitter Search Agents
Don't just follow rules â€” they *evaluate*:
- How noisy a domain is
- Whether depth is sufficient
- If second-level validation is required

### ðŸ§ª Source Review Agents
Actively rerank or prune content if trust scores fall short, pushing quality higher through intelligent evaluation.

## ðŸ›  Technical Architecture

### Core Components

1. ðŸ§  **Query Refiner**  
   - Builds multi-component research plans
   - Considers user role and goals
   - Sets update frequency parameters

2. ðŸ§­ **Router v0_4**  
   - Maps query scope to modality
   - Assigns sourcing budgets
   - Manages independent agents

3. âœï¸ **Writer Agent**  
   - Synthesizes modular content
   - Auto-queries for clarification
   - Enables source re-ranking

4. âš¡ **Live Agent Graph UI**  
   - SSE-driven real-time updates
   - Visual decision tracing
   - Interactive feedback system

## ðŸ›  Backend Architecture

### Core Components

1. **Query Processor Pipeline**
   - Query Refinement (Claude 3 Sonnet)
   - Web Search Agent
   - Twitter Search Agent
   - Source Review & Reranking
   - Router_04 for Agent Orchestration
   - Draft Generation

2. **State Management**
   - Real-time SSE progress streaming
   - Session persistence
   - Source caching and reranking

3. **Database Schema**
   - Queries table with JSONB for source storage
   - Drafts with versioning
   - User profiles and preferences

### API Design

1. **Query Flow**
   ```
   POST /queries          # Create new query
   GET  /queries/{id}     # Get query status
   POST /queries/{id}/refine  # Start refinement
   GET  /queries/stream/{id}  # SSE progress updates
   ```

2. **Source Review**
   ```
   GET  /queries/{id}/sources      # Get sources for review
   POST /queries/{id}/sources/review  # Submit reviewed sources
   ```

3. **Draft Management**
   ```
   POST /drafts/generate  # Generate from sources
   GET  /drafts/{id}      # Get draft content
   POST /drafts/{id}/accept  # Accept draft
   POST /drafts/{id}/reject  # Reject with feedback
   GET  /drafts/stream/{id}  # Stream generation
   ```

### Data Flow

1. **Query Processing**
   ```mermaid
   graph TD
   A[Raw Query] --> B[Query Refinement]
   B --> C[Web Search]
   B --> D[Twitter Search]
   C --> E[Source Review]
   D --> E
   E --> F[Router_04]
   F --> G[Draft Generation]
   ```

2. **Source Processing**
   ```mermaid
   graph TD
   A[Raw Sources] --> B[Trust Scoring]
   B --> C[User Review]
   C --> D[Final Reranking]
   D --> E[Router_04]
   ```

### Real-time Updates

The backend uses Server-Sent Events (SSE) to provide real-time updates on:
- Query refinement progress
- Source collection status
- Source review readiness
- Draft generation progress

Events are emitted in the format:
```json
{
  "stage": "ProcessStage",
  "timestamp": "datetime",
  "data": { stage-specific data }
}
```

### Technologies Used

- **FastAPI** - Asynchronous API framework
- **PostgreSQL** - JSONB storage for flexible document handling
- **SSE** - Real-time event streaming
- **OpenRouter** - Model provider abstraction and fallback
- **Claude 3 Sonnet** - Primary LLM for refinement and generation
- **JWT** - Authentication and session management

## ðŸ’» Development Setup

### Prerequisites

#### Frontend
- Node.js (v18+)
- npm (v9+) or yarn
- Visual Studio Code (recommended)

#### Backend
- Python 3.10+
- PostgreSQL
- Google OAuth credentials

### Installation

1. Clone the repository:
```bash
git clone https://github.com/your-username/vizier.git
cd vizier
```

2. Backend Setup:
```bash
# Create virtual environment
python -m venv venv

# Activate it
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

3. Configure Backend Environment:
Create a `.env` file:
```
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=
REDIRECT_URI=http://localhost:8000/auth/callback
FRONTEND_URL=http://localhost:3000
JWT_SECRET=
DATABASE_URL=
sslmode=require
```

4. Frontend Setup:
```bash
cd frontend
npm install
```

### Running the Application

#### Development Mode

Backend:
```bash
uvicorn main:app --reload
```

Frontend:
```bash
npm run dev
# or
yarn dev
```

Access the application at `http://localhost:5173`

## Backend Development Setup

1. Set up PostgreSQL:
   ```bash
   createdb vizier
   ```

2. Configure environment:
   ```bash
   cp example.env .env
   # Edit .env with your credentials:
   # - DATABASE_URL
   # - JWT_SECRET
   # - OPENROUTER_API_KEY
   ```

3. Initialize database:
   ```bash
   python -m alembic upgrade head
   ```

4. Run development server:
   ```bash
   uvicorn main:app --reload
   ```

## ðŸ“‚ Project Structure

```
vizier-frontend/
â”œâ”€â”€ public/                # Static files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ pages/         # Application pages
â”‚   â”‚   â”‚   â”œâ”€â”€ discover/  # Discover page
â”‚   â”‚   â”‚   â”œâ”€â”€ graph/     # Graph visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ library/   # Library page
â”‚   â”‚   â”‚   â”œâ”€â”€ login/     # Login and authentication
â”‚   â”‚   â”‚   â”œâ”€â”€ onboarding/# User onboarding
â”‚   â”‚   â”‚   â”œâ”€â”€ query/     # Query interface
â”‚   â”‚   â”‚   â”œâ”€â”€ settings/  # Settings page
â”‚   â”‚   â”‚   â””â”€â”€ spaces/    # Spaces page
â”‚   â”‚   â”œâ”€â”€ App.tsx        # Main application component
â”‚   â”‚   â”œâ”€â”€ App.css        # Main application styles
â”‚   â”‚   â”œâ”€â”€ index.css      # Global styles
â”‚   â”‚   â””â”€â”€ main.tsx       # Application entry point
â”‚   â”œâ”€â”€ components/        # Reusable components
â”‚   â”‚   â”œâ”€â”€ navigation/    # Navigation components
â”‚   â”‚   â””â”€â”€ querybar/      # Query bar components
â”‚   â””â”€â”€ vite-env.d.ts      # Vite environment typings
â”œâ”€â”€ index.html             # HTML entry point
â”œâ”€â”€ tsconfig.json          # TypeScript configuration
â”œâ”€â”€ tsconfig.app.json      # App-specific TypeScript config
â”œâ”€â”€ tsconfig.node.json     # Node-specific TypeScript config
â”œâ”€â”€ vite.config.ts         # Vite configuration
â”œâ”€â”€ package.json           # Project dependencies and scripts
â””â”€â”€ README.md              # Project documentation
```

## ðŸ”Œ API Integration

Refer to our [API Documentation](docs/API.md) for detailed endpoint specifications and integration guides.

## ðŸ¤ Contributing

1. Fork the repository
2. Create your feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a pull request

## ðŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


--------------------------------------------------
File End
--------------------------------------------------


.\requirements.txt
File type: .txt
fastapi
uvicorn[standard]

# async database and PostgreSQL
databases[asyncpg]
asyncpg

# Google OAuth
httpx

# JWT handling
python-jose[cryptography]

# Environment variable support
python-dotenv

--------------------------------------------------
File End
--------------------------------------------------


.\test.py
File type: .py
import asyncio
from routers.openrouter import OpenRouterClient

async def test_chat_completion():
    client = OpenRouterClient()
    try:
        response = await client.chat_completion(
            model="openrouter/optimus-alpha",
            messages=[
                {
                "role": "user",
                "content": "What's the capital of France?"
                }
            ],
            max_tokens=50,
            temperature=0.7
        )
        print("Response:\n", response)

    except Exception as e:
        print("Error:", e)
    finally:
        await client.client.aclose() # close AsyncClient

if __name__ == "__main__":
    asyncio.run(test_chat_completion())

--------------------------------------------------
File End
--------------------------------------------------


.\processes\main.py
File type: .py
"""Main module for coordinating writing flow

This module orchestrates the flow between Router_04, source agents, director and writer.
"""

import asyncio
from typing import Dict, Any, List
from pydantic import BaseModel

from processes.connectors.router_04 import Router0_4, WritingContextRequest
from processes.sourcing.director import SourceDirector
from processes.sourcing.agent import SourceAgent
from processes.report.writer import Writer, WriterRequest

async def initialize_writing_pipeline(router_request: WritingContextRequest) -> Dict[str, Any]:
    """Initialize and connect all components of the writing pipeline"""
    
    # 1. Initialize Router_04 and get writing context
    router = Router0_4(
        router_request.cleaned_web_sources,
        router_request.cleaned_twitter_sources,
        router_request.user_context
    )
    writing_context = await router.prepare_writing_context()

    # 2. Initialize source agents through director
    director = SourceDirector()
    
    # Create and register source agents based on router assignments
    for agent_id, agent_info in writing_context.source_agents.items():
        # Get source URLs for this agent
        source_urls = [
            writing_context.reranked_sources[sid].metadata.url
            for sid in agent_info.assigned_sources
            if sid in writing_context.reranked_sources
        ]
        
        # Create agent with role-specific context
        agent = SourceAgent(
            meta_prompt=f"You are source agent {agent_id} specializing in themes: {agent_info.source_types}",
            source_urls=source_urls,
            role_context=f"Focus on sources of type: {agent_info.source_types}",
            objectives=[
                "Extract key insights relevant to query",
                "Identify connections between sources",
                "Prepare to answer clarification requests"
            ]
        )
        
        # Register with director
        await director.register_agent(
            agent_id,
            agent,
            agent_info.assigned_sources
        )

    # 3. Initialize writer with complete context
    writer = Writer()
    
    # 4. Create writer request with full context
    writer_request = WriterRequest(
        writing_context_id=writing_context.writing_context_id,
        refined_query=writing_context.refined_query,
        context_summary=writing_context.context_summary,
        source_agents=writing_context.source_agents,  
        reranked_sources=writing_context.reranked_sources,
        thematic_clusters=writing_context.thematic_clusters,
        report_style="technical",  # Can be parameterized
        user_background=router_request.user_context.get("user_background", {})
    )
    
    return {
        "writing_context": writing_context,
        "director": director,
        "writer": writer,
        "writer_request": writer_request
    }

async def execute_writing_pipeline(components: Dict[str, Any]) -> Dict[str, Any]:
    """Execute the full writing pipeline with the initialized components"""
    
    writer = components["writer"]
    writer_request = components["writer_request"]
    
    # Generate initial draft
    response = await writer.generate_draft(writer_request)
    
    return {
        "draft_id": response.draft_id,
        "report_draft": response.report_draft,
        "suggested_improvements": response.suggested_improvements
    }

async def main():
    """Main entry point"""
    # Example usage
    router_request = WritingContextRequest(
        routing_id="example_routing",
        cleaned_web_sources={},  # Add sample sources
        cleaned_twitter_sources={},
        user_context={"user_background": {"user_type": "researcher"}}
    )
    
    try:
        # Initialize pipeline
        components = await initialize_writing_pipeline(router_request)
        
        # Execute pipeline
        result = await execute_writing_pipeline(components)
        
        print(f"Generated draft {result['draft_id']}")
        print(f"Suggested improvements: {result['suggested_improvements']}")
        
    except Exception as e:
        print(f"Error in writing pipeline: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

--------------------------------------------------
File End
--------------------------------------------------


.\processes\connectors\router_0.py
File type: .py
"""
Router Planning Flow Module

This module implements the core routing functionality for Vizier's research planning process.
It contains the Router0 agent-centric router that creates rich search contexts for external 
retrieval agents. Instead of hardcoded rules, it focuses on generating guidance that helps agents 
understand the search intent, quality requirements, and context necessary for high-quality 
research.

Router0 is the first step in the research workflow after query refinement, responsible for
creating specialized search contexts for different agent types before data collection begins.
"""

import os, asyncio, json
from typing import Dict, List, Optional, Any
from enum import Enum
from datetime import datetime
from pydantic import BaseModel, Field
from routers.openrouter import OpenRouterClient

# constants
ROUTER_MODEL = "openrouter/mistralai/mixtral-8x7b-instruct" # can use a smaller model for routing


class SourceType(str, Enum):
    """Types of external data sources"""
    WEB = "web"
    TWITTER = "twitter"
    ACADEMIC = "academic"
    NEWS = "news"
    BLOG = "blog"
    FORUM = "forum"
    OTHER = "other"

class SourceRelevance(str, Enum):
    """Relevance categories for sources"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    IRRELEVANT = "irrelevant"


class UserBackground(BaseModel):
    """User background information for context routing"""
    user_type: str = Field(description="Type of user (e.g., 'Specialized Professional')")
    research_purpose: str = Field(description="What the user will use Vizier for")
    user_description: str = Field(description="Brief description of who the user is")
    query_frequency: str = Field(description="How often the query will be run (daily/weekly/monthly)")


class SourceExplorationGuidance(BaseModel):
    """Agent-centric guidance for source exploration"""
    overall_strategy: str = Field(description="High-level search strategy guidance")
    source_priorities: Dict[str, float] = Field(description="Relative importance of different source types")
    quality_indicators: Dict[str, List[str]] = Field(description="What indicates quality for each source type")
    depth_guidance: str = Field(description="Guidance on technical depth")
    recency_guidance: str = Field(description="Guidance on source recency")
    authority_guidance: str = Field(description="How to evaluate source authority")
    special_considerations: Optional[str] = Field(None, description="Special research considerations")


class RoutingRequest(BaseModel):
    """Request model for routing a refined query"""
    refined_query: str = Field(description="The refined query to route")
    background: UserBackground = Field(description="User background information")
    domain_context: Optional[Dict[str, Any]] = Field(None, description="Additional domain context")
    exploration_preferences: Optional[Dict[str, Any]] = Field(None, description="User preferences for exploration")

class RoutingResponse(BaseModel):
    """Response model for routing results"""
    routing_id: str = Field(description="Unique identifier for this routing operation")
    search_guidance: SourceExplorationGuidance = Field(description="Agent-centric search guidance")
    web_agent_context: Dict[str, Any] = Field(description="Context for web search agents")
    twitter_agent_context: Dict[str, Any] = Field(description="Context for Twitter search agents")
    academic_agent_context: Optional[Dict[str, Any]] = Field(None, description="Context for academic search agents")


class Router0:
    """
    Agent-centric router that provides strategic guidance for external data collection agents.
    
    Instead of hardcoded rules and keyword extraction, Router0 uses LLM capabilities to:
    1. Understand the query intent and user needs
    2. Generate rich guidance for downstream search agents
    3. Create specialized contexts for different agent types
    """

    def __init__(self, model: str = ROUTER_MODEL):
        """Initialize the Router0 instance with an LLM client"""
        self.client = OpenRouterClient()
        self.model = model

    def _generate_routing_id(self) -> str:
        """Generate a unique identifier for the routing operation"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        random_part = os.urandom(4).hex()
        return f"route_{timestamp}_{random_part}"
    
    async def _generate_search_guidance(self, query: str, background: UserBackground, 
                                      domain_context: Optional[Dict[str, Any]] = None) -> SourceExplorationGuidance:
        """
        Use LLM to generate search guidance for external agents.
        
        Args:
            query: The refined query
            background: User background information
            domain_context: Optional additional domain context
            
        Returns:
            SourceExplorationGuidance with agent-centric search recommendations
        """
        # Create the prompt for search guidance
        prompt = f"""
        You are an expert research strategist helping plan a search strategy for a complex query.

        # QUERY DETAILS
        Refined Query: {query}

        # USER CONTEXT
        User Type: {background.user_type}
        Research Purpose: {background.research_purpose}
        User Description: {background.user_description}
        Query Frequency: {background.query_frequency}

        # TASK
        Create a detailed search strategy that will help guide intelligent search agents to find the most valuable sources for this query.
        
        Provide guidance on:
        1. Overall search strategy (key objectives, approach)
        2. Source type priorities (assign a value from 0.0-1.0 to each of: WEB, TWITTER, ACADEMIC, NEWS, BLOG, FORUM)
        3. Quality indicators for each source type
        4. Technical depth requirements
        5. Recency considerations
        6. Authority evaluation criteria
        7. Any special considerations for this specific query

        # OUTPUT FORMAT
        Provide a JSON object with the following structure:
        ```json
        {{
            "overall_strategy": "Clear strategic guidance for search agents",
            "source_priorities": {{
                "WEB": 0.9,
                "TWITTER": 0.7,
                "ACADEMIC": 0.8,
                "NEWS": 0.6,
                "BLOG": 0.5,
                "FORUM": 0.4
            }},
            "quality_indicators": {{
                "WEB": ["indicator1", "indicator2", "indicator3"],
                "TWITTER": ["indicator1", "indicator2", "indicator3"],
                "ACADEMIC": ["indicator1", "indicator2", "indicator3"]
            }},
            "depth_guidance": "Guidance on required technical depth",
            "recency_guidance": "Guidance on source recency",
            "authority_guidance": "How to evaluate source authority",
            "special_considerations": "Any special recommendations for this query"
        }}
        ```

        Focus on providing actionable, specific guidance that helps agents make intelligent decisions about source quality and relevance.
        """
        
        # If we have domain context, add it to the prompt
        if domain_context:
            domain_context_str = json.dumps(domain_context, indent=2)
            prompt += f"\n\n# DOMAIN CONTEXT\n{domain_context_str}"
        
        # Call the LLM to generate the search guidance
        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2,  # Low temperature for more consistent results
                max_tokens=1500,
            )
            
            # Extract the JSON response
            guidance_json = json.loads(response["choices"][0]["message"]["content"])
            
            # Create and return the SourceExplorationGuidance object
            return SourceExplorationGuidance(
                overall_strategy=guidance_json["overall_strategy"],
                source_priorities=guidance_json["source_priorities"],
                quality_indicators=guidance_json["quality_indicators"],
                depth_guidance=guidance_json["depth_guidance"],
                recency_guidance=guidance_json["recency_guidance"],
                authority_guidance=guidance_json["authority_guidance"],
                special_considerations=guidance_json.get("special_considerations")
            )
            
        except Exception as e:
            # Fallback values if the LLM call fails
            print(f"Error generating search guidance: {e}")
            return SourceExplorationGuidance(
                overall_strategy=f"Find high-quality, relevant sources for: {query}",
                source_priorities={"WEB": 0.9, "TWITTER": 0.7, "ACADEMIC": 0.8, 
                                  "NEWS": 0.6, "BLOG": 0.5, "FORUM": 0.4},
                quality_indicators={
                    "WEB": ["Authoritative source", "Comprehensive coverage", "Technical accuracy"],
                    "TWITTER": ["Expert authors", "Substantive threads", "Evidence-backed claims"],
                    "ACADEMIC": ["Peer-reviewed", "Cited by experts", "Rigorous methodology"]
                },
                depth_guidance="Match technical depth to user background and query complexity",
                recency_guidance="Prioritize recent sources unless seeking foundational knowledge",
                authority_guidance="Seek sources from recognized experts and institutions",
                special_considerations=None
            )
    
    async def _generate_web_search_context(self, query: str, background: UserBackground, 
                                        guidance: SourceExplorationGuidance) -> Dict[str, Any]:
        """
        Generate a context object for web search agents.
        
        Args:
            query: The refined query
            background: User background information
            guidance: Generated search guidance
            
        Returns:
            Context dictionary for web search agents
        """
        # Create the prompt for web search context
        prompt = f"""
        You are an expert search strategist creating a search plan for web sources.
        
        # QUERY
        {query}
        
        # USER CONTEXT
        User Type: {background.user_type}
        Research Purpose: {background.research_purpose}
        
        # SEARCH GUIDANCE
        Overall Strategy: {guidance.overall_strategy}
        Web Source Priority: {guidance.source_priorities.get("WEB", 0.8)}/1.0
        Depth Guidance: {guidance.depth_guidance}
        Recency Guidance: {guidance.recency_guidance}
        Authority Guidance: {guidance.authority_guidance}
        
        Web Quality Indicators:
        {json.dumps(guidance.quality_indicators.get("WEB", []), indent=2)}
        
        # TASK
        Create a web search context object that will guide a web search agent. Include:
        1. Search parameters (timeframe, filters, etc.)
        2. Priority topics/aspects to investigate
        3. Specific domains or source types to prioritize
        4. Guidelines for evaluating source quality
        5. Any specific search strategies that would be effective
        
        # OUTPUT FORMAT
        Provide ONLY a JSON object with search parameters and guidance:
        ```json
        {{
            "search_parameters": {{
                "timeframe": "...",
                "filters": [...],
                "excluded_domains": [...],
                "preferred_domains": [...]
            }},
            "priority_topics": [...],
            "depth_requirements": "...",
            "quality_evaluation": [...],
            "search_strategies": [...]
        }}
        ```
        """
        
        # Call the LLM to generate the web search context
        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_tokens=1200,
            )
            
            # Extract and return the JSON response
            web_context = json.loads(response["choices"][0]["message"]["content"])
            return web_context
            
        except Exception as e:
            # Fallback values if the LLM call fails
            print(f"Error generating web search context: {e}")
            return {
                "search_parameters": {
                    "timeframe": "past year",
                    "filters": [],
                    "excluded_domains": [],
                    "preferred_domains": []
                },
                "priority_topics": [query],
                "depth_requirements": guidance.depth_guidance,
                "quality_evaluation": guidance.quality_indicators.get("WEB", []),
                "search_strategies": ["Use exact phrases from the query"]
            }
    
    async def _generate_twitter_search_context(self, query: str, background: UserBackground, 
                                           guidance: SourceExplorationGuidance) -> Dict[str, Any]:
        """
        Generate a context object for Twitter search agents.
        
        Args:
            query: The refined query
            background: User background information
            guidance: Generated search guidance
            
        Returns:
            Context dictionary for Twitter search agents
        """
        # Create the prompt for Twitter search context
        prompt = f"""
        You are an expert social media search strategist creating a search plan for Twitter.
        
        # QUERY
        {query}
        
        # USER CONTEXT
        User Type: {background.user_type}
        Research Purpose: {background.research_purpose}
        
        # SEARCH GUIDANCE
        Overall Strategy: {guidance.overall_strategy}
        Twitter Source Priority: {guidance.source_priorities.get("TWITTER", 0.7)}/1.0
        Depth Guidance: {guidance.depth_guidance}
        Recency Guidance: {guidance.recency_guidance}
        Authority Guidance: {guidance.authority_guidance}
        
        Twitter Quality Indicators:
        {json.dumps(guidance.quality_indicators.get("TWITTER", []), indent=2)}
        
        # TASK
        Create a Twitter search context object that will guide a Twitter search agent. Include:
        1. Search parameters (timeframe, engagement thresholds, etc.)
        2. Account types to prioritize 
        3. Relevant hashtags for the query
        4. Guidelines for evaluating tweet and thread quality
        5. Any specific Twitter search strategies that would be effective
        
        # OUTPUT FORMAT
        Provide ONLY a JSON object with search parameters and guidance:
        ```json
        {{
            "search_parameters": {{
                "timeframe": "...",
                "min_engagement": ...,
                "verified_only": true/false,
                "exclude_terms": [...]
            }},
            "priority_accounts": [...],
            "relevant_hashtags": [...],
            "quality_evaluation": [...],
            "search_strategies": [...]
        }}
        ```
        """
        
        # Call the LLM to generate the Twitter search context
        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_tokens=1200,
            )
            
            # Extract and return the JSON response
            twitter_context = json.loads(response["choices"][0]["message"]["content"])
            return twitter_context
            
        except Exception as e:
            # Fallback values if the LLM call fails
            print(f"Error generating Twitter search context: {e}")
            return {
                "search_parameters": {
                    "timeframe": "past week",
                    "min_engagement": 5,
                    "verified_only": False,
                    "exclude_terms": ["spam", "giveaway", "promotion"]
                },
                "priority_accounts": [],
                "relevant_hashtags": [],
                "quality_evaluation": guidance.quality_indicators.get("TWITTER", []),
                "search_strategies": ["Use key terms from query"]
            }
    
    async def _generate_academic_search_context(self, query: str, background: UserBackground, 
                                            guidance: SourceExplorationGuidance) -> Optional[Dict[str, Any]]:
        """
        Generate a context object for academic search agents if needed.
        
        Args:
            query: The refined query
            background: User background information
            guidance: Generated search guidance
            
        Returns:
            Context dictionary for academic search agents, or None if not needed
        """
        # Check if academic sources are important for this query
        academic_priority = guidance.source_priorities.get("ACADEMIC", 0.0)
        if academic_priority < 0.5:
            return None
            
        # Create the prompt for academic search context
        prompt = f"""
        You are an expert academic research strategist creating a search plan for scholarly sources.
        
        # QUERY
        {query}
        
        # USER CONTEXT
        User Type: {background.user_type}
        Research Purpose: {background.research_purpose}
        
        # SEARCH GUIDANCE
        Overall Strategy: {guidance.overall_strategy}
        Academic Source Priority: {academic_priority}/1.0
        Depth Guidance: {guidance.depth_guidance}
        Recency Guidance: {guidance.recency_guidance}
        Authority Guidance: {guidance.authority_guidance}
        
        Academic Quality Indicators:
        {json.dumps(guidance.quality_indicators.get("ACADEMIC", []), indent=2)}
        
        # TASK
        Create an academic search context object that will guide a scholarly search agent. Include:
        1. Search parameters (publication date range, citation thresholds, etc.)
        2. Key journals or conferences to prioritize
        3. Relevant academic fields/disciplines
        4. Guidelines for evaluating paper quality and relevance
        5. Any specific academic search strategies that would be effective
        
        # OUTPUT FORMAT
        Provide ONLY a JSON object with search parameters and guidance:
        ```json
        {{
            "search_parameters": {{
                "date_range": "...",
                "min_citations": ...,
                "open_access_preferred": true/false,
                "include_preprints": true/false
            }},
            "priority_venues": [...],
            "relevant_fields": [...],
            "quality_evaluation": [...],
            "search_strategies": [...]
        }}
        ```
        """
        
        # Call the LLM to generate the academic search context
        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_tokens=1200,
            )
            
            # Extract and return the JSON response
            academic_context = json.loads(response["choices"][0]["message"]["content"])
            return academic_context
            
        except Exception as e:
            # Fallback values if the LLM call fails
            print(f"Error generating academic search context: {e}")
            return {
                "search_parameters": {
                    "date_range": "past 3 years",
                    "min_citations": 0,
                    "open_access_preferred": True,
                    "include_preprints": True
                },
                "priority_venues": [],
                "relevant_fields": [],
                "quality_evaluation": guidance.quality_indicators.get("ACADEMIC", []),
                "search_strategies": ["Use technical terms from query"]
            }
    
    async def route_query(self, request: RoutingRequest) -> RoutingResponse:
        """
        Route a refined query by providing strategic guidance for external retrieval agents.

        Args:
            request: The routing request containing query and user background
    
        Returns:
            Routing response with exploration strategies and context
        """
        # Generate a unique routing ID
        routing_id = self._generate_routing_id()
        
        # Generate search guidance (agent-centric, not hardcoded rules)
        search_guidance = await self._generate_search_guidance(
            request.refined_query,
            request.background,
            request.domain_context
        )
        
        # Generate contexts for different agent types in parallel
        web_context_task = self._generate_web_search_context(
            request.refined_query,
            request.background,
            search_guidance
        )
        
        twitter_context_task = self._generate_twitter_search_context(
            request.refined_query,
            request.background,
            search_guidance
        )
        
        academic_context_task = self._generate_academic_search_context(
            request.refined_query,
            request.background,
            search_guidance
        )
        
        # Wait for all context generation tasks to complete
        web_context, twitter_context, academic_context = await asyncio.gather(
            web_context_task,
            twitter_context_task,
            academic_context_task
        )
        
        # Create and return the routing response
        return RoutingResponse(
            routing_id=routing_id,
            search_guidance=search_guidance,
            web_agent_context=web_context,
            twitter_agent_context=twitter_context,
            academic_agent_context=academic_context
        )


async def route_query(request: RoutingRequest) -> RoutingResponse:
    """
    API endpoint for routing a refined query to data sources.
    
    Args:
        request: The routing request
        
    Returns:
        RoutingResponse with exploration strategies and context
    """
    router = Router0()
    response = await router.route_query(request)
    await router.client.client.aclose()  # Close the client connection
    return response


# Example usage function for testing
async def test_router():
    """Test the router with a sample query"""
    # Set up a sample request
    request = RoutingRequest(
        refined_query="Recent advances in transformer architecture efficiency improvements",
        background=UserBackground(
            user_type="Specialized Professional",
            research_purpose="Staying updated on AI research developments",
            user_description="ML researcher focusing on NLP and transformer models",
            query_frequency="weekly"
        )
    )
    
    # Test Router0
    router = Router0()
    try:
        print("Testing Router0...")
        response = await router.route_query(request)
        
        print(f"Routing ID: {response.routing_id}")
        print(f"Search Guidance:")
        print(f"  Overall Strategy: {response.search_guidance.overall_strategy}")
        print(f"  Source Priorities: {response.search_guidance.source_priorities}")
        print(f"  Depth Guidance: {response.search_guidance.depth_guidance}")
        print(f"  Recency Guidance: {response.search_guidance.recency_guidance}")
        
        print("\nWeb Agent Context:")
        print(json.dumps(response.web_agent_context, indent=2))
        
        print("\nTwitter Agent Context:")
        print(json.dumps(response.twitter_agent_context, indent=2))
        
        if response.academic_agent_context:
            print("\nAcademic Agent Context:")
            print(json.dumps(response.academic_agent_context, indent=2))
            
    except Exception as e:
        print(f"Error testing Router0: {e}")
    finally:
        await router.client.client.aclose()
        
if __name__ == "__main__":
    try:
        asyncio.run(test_router())
    except KeyboardInterrupt:
        print("\nExiting router test.")
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()


--------------------------------------------------
File End
--------------------------------------------------


.\processes\connectors\router_04.py
File type: .py
"""
Writing Context Router Module

This module implements Router0_4, which specializes in preparing context for the writing process 
by handling quality control, reranking, and source assignment for external data that has
been collected and cleaned.

The Router0_4 takes the output from data collection agents and transforms it into
an optimized structure for downstream writing modules by:
1. Performing quality control through reranking and filtering
2. Organizing sources into thematically coherent groups
3. Assigning sources to processing agents based on themes
4. Creating a rich context summary for writing modules
"""

import os, json
from typing import Dict, List, Optional, Any
from enum import Enum
from datetime import datetime
from pydantic import BaseModel, Field
from collections import Counter


# constants
DEFAULT_QUALITY_THRESHOLD = 0.6 # min quality score (0-1) to include a source
MIN_SOURCE_AGENTS = 3
MAX_SOURCE_AGENTS = 15


class SourceType(str, Enum):
    """Types of external data sources"""
    WEB = "web"
    TWITTER = "twitter"
    ACADEMIC = "academic"
    NEWS = "news"
    BLOG = "blog"
    FORUM = "forum"
    OTHER = "other"

class SourceRelevance(str, Enum):
    """Relevance categories for sources"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    IRRELEVANT = "irrelevant"


class UserBackground(BaseModel):
    """User background information for context routing"""
    user_type: str = Field(description="Type of user (e.g., 'Specialized Professional')")
    research_purpose: str = Field(description="What the user will use Vizier for")
    user_description: str = Field(description="Brief description of who the user is")
    query_frequency: str = Field(description="How often the query will be run (daily/weekly/monthly)")

class SourceMetadata(BaseModel):
    """Metadata for an external source"""
    source_id: str = Field(description="Unique identifier for the source")
    source_type: SourceType = Field(description="Type of source")
    url: Optional[str] = Field(None, description="URL of the source if applicable")
    title: Optional[str] = Field(None, description="Title of the source")
    author: Optional[str] = Field(None, description="Author of the source")
    publication_date: Optional[str] = Field(None, description="Publication date of the source")
    retrieved_date: str = Field(description="Date when the source was retrieved")
    relevance_score: float = Field(description="Relevance score (0-1)")
    quality_score: float = Field(description="Quality score (0-1)")
    content_snippet: Optional[str] = Field(None, description="Brief snippet of the source content")

class CleanedSource(BaseModel):
    """A cleaned external source with content and metadata"""
    metadata: SourceMetadata = Field(description="Metadata about the source")
    content: str = Field(description="Cleaned content of the source")
    keywords: List[str] = Field(description="Extracted keywords from the source")
    relevance: SourceRelevance = Field(description="Assessed relevance to the query")


class SourceAgent(BaseModel):
    """Representation of a source processing agent"""
    agent_id: str = Field(description="Unique identifier for the agent")
    assigned_sources: List[str] = Field(description="List of source IDs assigned to this agent")
    source_types: List[SourceType] = Field(description="Types of sources assigned to this agent")
    priority: int = Field(description="Processing priority (lower = higher priority)")


class WritingContextRequest(BaseModel):
    """Request model for preparing writing context"""
    routing_id: str = Field(description="ID of the initial routing operation")
    cleaned_web_sources: Dict[str, CleanedSource] = Field(description="Cleaned web sources")
    cleaned_twitter_sources: Dict[str, CleanedSource] = Field(description="Cleaned Twitter sources")
    user_context: Dict[str, Any] = Field(description="User context information")

class WritingContextResponse(BaseModel):
    """Response model with prepared writing context"""
    writing_context_id: str = Field(description="Unique identifier for this writing context")
    refined_query: str = Field(description="The refined query")
    reranked_sources: Dict[str, CleanedSource] = Field(description="Reranked and filtered sources")
    source_agents: Dict[str, SourceAgent] = Field(description="Source agents and their assignments")
    thematic_clusters: Dict[str, List[str]] = Field(description="Sources grouped by theme/topic")
    context_summary: Dict[str, Any] = Field(description="Summary of the writing context")


class Router0_4:
    """
    Specialized router for writing preparation. It takes cleaned external sources,
    performs quality control through reranking and filtering, and organizes sources
    into thematically coherent groups for optimal writing context.
    
    This router preserves rich context while ensuring writing modules receive
    only high-quality, relevant information organized in a useful structure.
    
    Key responsibilities:
    1. Input Aggregation: Receives cleaned source dictionaries from upstream processes
    2. Quality Control: Filters low-quality sources and reranks based on relevance
    3. Thematic Organization: Clusters sources by topic for coherent context
    4. Agent Assignment: Distributes sources among processing agents by theme
    5. Context Sanitization: Prepares a clean, structured context for writing modules
    """
    
    def __init__(
        self,
        cleaned_web_sources: Dict[str, CleanedSource],
        cleaned_twitter_sources: Dict[str, CleanedSource],
        user_context: Dict[str, Any],
        quality_threshold: float = DEFAULT_QUALITY_THRESHOLD
    ):
        """
        Initialize the Router0_4 instance.
        
        Args:
            cleaned_web_sources: Dictionary of cleaned web sources
            cleaned_twitter_sources: Dictionary of cleaned Twitter sources
            user_context: User context information including refined query
            quality_threshold: Minimum quality score to include a source
        """
        self.cleaned_web_sources = cleaned_web_sources
        self.cleaned_twitter_sources = cleaned_twitter_sources
        self.user_context = user_context
        self.quality_threshold = quality_threshold
        self.reranked_sources = None
        self.assigned_agents = None
        self.thematic_clusters = None
        
    def _generate_writing_context_id(self) -> str:
        """Generate a unique identifier for the writing context"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        random_part = os.urandom(4).hex()
        return f"write_{timestamp}_{random_part}"
    
    def _extract_thematic_keywords(self, source: CleanedSource) -> List[str]:
        """
        Extract thematic keywords from a source.
        
        Args:
            source: The source to analyze
            
        Returns:
            List of thematic keywords
        """
        # Start with source's own keywords
        keywords = source.keywords.copy() if source.keywords else []
        
        # Add title words if available
        if source.metadata.title:
            title_words = [word.lower() for word in source.metadata.title.split() 
                          if len(word) > 3 and word.lower() not in ["the", "and", "for"]]
            keywords.extend(title_words)
            
        # Extract from content snippet if available
        if source.metadata.content_snippet:
            snippet_words = [word.lower() for word in source.metadata.content_snippet.split() 
                            if len(word) > 4 and word.lower() not in ["the", "and", "for", "that", "with"]]
            # Take the most frequent words from the snippet
            word_counts = Counter(snippet_words)
            top_words = [word for word, _ in word_counts.most_common(5)]
            keywords.extend(top_words)
            
        # Remove duplicates and return
        unique_keywords = list(set(keywords))
        return unique_keywords[:10]  # Limit to top 10 keywords
    
    def rerank_sources(self) -> Dict[str, CleanedSource]:
        """
        Rerank and filter sources based on relevance, quality and complementary value.
        
        This prioritizes high-quality sources while ensuring diversity of information
        and complementary perspectives.
        
        Returns:
            Dictionary of reranked and filtered sources
        """
        # Combine web and Twitter sources
        combined_sources = {**self.cleaned_web_sources, **self.cleaned_twitter_sources}
        
        # Filter out low-quality sources
        filtered_sources = {
            source_id: source 
            for source_id, source in combined_sources.items()
            if source.metadata.quality_score >= self.quality_threshold
        }
        
        # Extract refined query from user context
        refined_query = self.user_context.get("refined_query", "")
        
        # Create a scoring function that considers relevance, quality, and information diversity
        def source_score(source: CleanedSource) -> float:
            # Base weights
            relevance_weight = 0.6
            quality_weight = 0.3
            diversity_weight = 0.1
            
            # Convert relevance enum to numeric score
            relevance_numeric = {
                SourceRelevance.HIGH: 1.0,
                SourceRelevance.MEDIUM: 0.7,
                SourceRelevance.LOW: 0.4,
                SourceRelevance.IRRELEVANT: 0.0
            }.get(source.relevance, 0.0)
            
            # Calculate keyword overlap with query as a diversity signal
            query_words = set([w.lower() for w in refined_query.split() if len(w) > 3])
            source_keywords = set([k.lower() for k in self._extract_thematic_keywords(source)])
            
            # Unique keywords that aren't in the query indicate information diversity
            unique_keywords = source_keywords - query_words
            diversity_score = min(1.0, len(unique_keywords) / 5.0)  # Scale to 0-1
            
            # Calculate weighted score
            return (
                relevance_weight * relevance_numeric + 
                quality_weight * source.metadata.quality_score +
                diversity_weight * diversity_score
            )
        
        # Score and sort sources
        source_scores = {source_id: source_score(source) for source_id, source in filtered_sources.items()}
        
        # Rerank sources based on scores
        sorted_sources = dict(
            sorted(
                filtered_sources.items(), 
                key=lambda item: source_scores.get(item[0], 0),
                reverse=True
            )
        )
        
        self.reranked_sources = sorted_sources
        return sorted_sources
    
    def _cluster_sources_by_theme(self) -> Dict[str, List[str]]:
        """
        Group sources into thematic clusters based on keyword similarity.
        
        Returns:
            Dictionary mapping themes to lists of source IDs
        """
        if not self.reranked_sources:
            self.rerank_sources()
        
        # Extract keywords for each source
        source_keywords = {
            source_id: set(self._extract_thematic_keywords(source))
            for source_id, source in self.reranked_sources.items()
        }
        
        # Find common themes across sources
        all_keywords = set()
        for keywords in source_keywords.values():
            all_keywords.update(keywords)
        
        # Create thematic clusters
        themes = {}
        
        # Group by keyword overlap
        for keyword in all_keywords:
            # Find sources that contain this keyword
            sources_with_keyword = [
                source_id for source_id, keywords in source_keywords.items()
                if keyword in keywords
            ]
            
            if len(sources_with_keyword) > 1:  # Only create themes with multiple sources
                themes[keyword] = sources_with_keyword
        
        # Consolidate overlapping themes
        consolidated_themes = {}
        processed_themes = set()
        
        for theme, sources in sorted(themes.items(), key=lambda x: len(x[1]), reverse=True):
            if theme in processed_themes:
                continue
                
            # Find overlapping themes
            overlapping = [
                t for t in themes
                if t != theme and not t in processed_themes and 
                len(set(themes[t]) & set(sources)) / len(themes[t]) > 0.5
            ]
            
            # Mark as processed
            processed_themes.add(theme)
            for t in overlapping:
                processed_themes.add(t)
            
            # Create consolidated theme
            if overlapping:
                theme_name = f"{theme}+{len(overlapping)}"
                consolidated_sources = set(sources)
                for t in overlapping:
                    consolidated_sources.update(themes[t])
                consolidated_themes[theme_name] = list(consolidated_sources)
            else:
                consolidated_themes[theme] = sources
        
        self.thematic_clusters = consolidated_themes
        return consolidated_themes
        
    def assign_source_agents(self) -> Dict[str, SourceAgent]:
        """
        Assign sources to agents based on thematic clusters rather than
        simple mechanical distribution.
        
        Returns:
            Dictionary mapping agent IDs to SourceAgent objects
        """
        if not self.reranked_sources:
            self.rerank_sources()
            
        if not self.thematic_clusters:
            self._cluster_sources_by_theme()
            
        # Determine how many agents we need based on thematic clustering
        theme_count = len(self.thematic_clusters)
        agent_count = max(
            MIN_SOURCE_AGENTS, 
            min(MAX_SOURCE_AGENTS, (theme_count + 1) // 2)
        )
        
        # Create empty agents
        agents = {
            f"agent_{i+1}": SourceAgent(
                agent_id=f"agent_{i+1}",
                assigned_sources=[],
                source_types=[],
                priority=i+1
            )
            for i in range(agent_count)
        }
        
        # Sort themes by size (number of sources)
        sorted_themes = sorted(
            self.thematic_clusters.items(),
            key=lambda item: len(item[1]),
            reverse=True
        )
        
        # Assign entire themes to agents, distributing by theme size
        agent_keys = list(agents.keys())
        current_theme_sizes = {agent_key: 0 for agent_key in agent_keys}
        
        for theme, sources in sorted_themes:
            # Find agent with lowest current workload
            target_agent_key = min(
                current_theme_sizes.items(),
                key=lambda x: x[1]
            )[0]
            
            # Assign all sources in this theme to the agent
            agent = agents[target_agent_key]
            agent.assigned_sources.extend(sources)
            
            # Update agent source types
            for source_id in sources:
                if source_id in self.reranked_sources:
                    source_type = self.reranked_sources[source_id].metadata.source_type
                    if source_type not in agent.source_types:
                        agent.source_types.append(source_type)
            
            # Update theme size counter for this agent
            current_theme_sizes[target_agent_key] += len(sources)
        
        self.assigned_agents = agents
        return agents
        
    def _prepare_context_summary(self) -> Dict[str, Any]:
        """
        Prepare a rich context summary for the writing module.
        
        Unlike the previous implementation, this preserves valuable context
        while organizing it in a structured way for optimal writing.
        
        Returns:
            Dictionary containing the writing context summary
        """
        if not self.reranked_sources:
            self.rerank_sources()
            
        if not self.assigned_agents:
            self.assign_source_agents()
            
        if not self.thematic_clusters:
            self._cluster_sources_by_theme()
            
        # Extract the refined query
        refined_query = self.user_context.get("refined_query", "")
        
        # Extract key topics from the sources
        all_keywords = []
        for source in self.reranked_sources.values():
            all_keywords.extend(source.keywords)
        
        keyword_counter = Counter(all_keywords)
        top_topics = [topic for topic, _ in keyword_counter.most_common(10)]
        
        # Create source summaries with rich context
        source_summaries = {}
        for source_id, source in self.reranked_sources.items():
            source_summaries[source_id] = {
                "title": source.metadata.title,
                "source_type": source.metadata.source_type,
                "url": source.metadata.url,
                "author": source.metadata.author,
                "publication_date": source.metadata.publication_date,
                "relevance": source.relevance,
                "quality_score": source.metadata.quality_score,
                "content_snippet": source.metadata.content_snippet,
                "keywords": source.keywords,
                # Find which themes this source belongs to
                "themes": [
                    theme for theme, sources in self.thematic_clusters.items()
                    if source_id in sources
                ]
            }
        
        # Analyze source composition
        source_type_counts = {}
        for source in self.reranked_sources.values():
            source_type = source.metadata.source_type
            source_type_counts[source_type] = source_type_counts.get(source_type, 0) + 1
        
        # Create a summary of the agent assignments
        agent_summaries = {}
        for agent_id, agent in self.assigned_agents.items():
            # Find which themes this agent is responsible for
            agent_themes = []
            for theme, sources in self.thematic_clusters.items():
                if any(source_id in agent.assigned_sources for source_id in sources):
                    agent_themes.append(theme)
            
            agent_summaries[agent_id] = {
                "source_count": len(agent.assigned_sources),
                "assigned_sources": agent.assigned_sources,
                "source_types": agent.source_types,
                "priority": agent.priority,
                "themes": agent_themes
            }
        
        # Create the context summary
        context_summary = {
            "refined_query": refined_query,
            "top_topics": top_topics,
            "source_composition": source_type_counts,
            "sources": source_summaries,
            "thematic_clusters": self.thematic_clusters,
            "source_agents": agent_summaries,
            "user_context": {
                "user_type": self.user_context.get("user_background", {}).get("user_type", ""),
                "research_purpose": self.user_context.get("user_background", {}).get("research_purpose", ""),
                "query_frequency": self.user_context.get("user_background", {}).get("query_frequency", "")
            },
            "writing_guidance": {
                "structure_recommendation": "thematic" if len(self.thematic_clusters) >= 3 else "source_type",
                "technical_depth": "high" if self.user_context.get("user_background", {}).get("user_type", "") == "Specialized Professional" else "medium",
                "emphasis": top_topics[:3] if top_topics else []
            }
        }
        
        return context_summary
        
    async def prepare_writing_context(self) -> WritingContextResponse:
        """
        Execute the full Router0_4 workflow to prepare a rich, organized context
        for the writing module.
        
        Returns:
            WritingContextResponse with the prepared writing context
        """
        # Generate a unique writing context ID
        writing_context_id = self._generate_writing_context_id()
        
        # Run the reranking and filtering
        self.rerank_sources()
        
        # Cluster sources by theme
        self._cluster_sources_by_theme()
        
        # Assign sources to agents
        self.assign_source_agents()
        
        # Create rich context summary
        context_summary = self._prepare_context_summary()
        
        # Extract refined query
        refined_query = self.user_context.get("refined_query", "")
        
        return WritingContextResponse(
            writing_context_id=writing_context_id,
            refined_query=refined_query,
            reranked_sources=self.reranked_sources,
            source_agents=self.assigned_agents,
            thematic_clusters=self.thematic_clusters,
            context_summary=context_summary
        )


async def prepare_writing_context(request: WritingContextRequest) -> WritingContextResponse:
    """
    API endpoint for preparing writing context from cleaned sources.
    
    Args:
        request: The writing context request
        
    Returns:
        WritingContextResponse with prepared writing context
    """
    router = Router0_4(
        request.cleaned_web_sources,
        request.cleaned_twitter_sources,
        request.user_context
    )
    return await router.prepare_writing_context()


# Example usage function for testing
async def test_router04():
    """Test the Router0_4 with sample data"""
    from datetime import datetime
    
    # Sample cleaned web sources
    web_sources = {
        "web_1": CleanedSource(
            metadata=SourceMetadata(
                source_id="web_1",
                source_type=SourceType.WEB,
                url="https://example.com/article1",
                title="Recent Advances in Transformer Efficiency",
                author="J. Smith",
                publication_date="2023-01-15",
                retrieved_date=datetime.now().strftime("%Y-%m-%d"),
                relevance_score=0.92,
                quality_score=0.88,
                content_snippet="This article discusses recent advances in transformer architecture efficiency..."
            ),
            content="Full content of article about transformer efficiency...",
            keywords=["transformers", "efficiency", "deep learning", "attention", "optimization"],
            relevance=SourceRelevance.HIGH
        ),
        "web_2": CleanedSource(
            metadata=SourceMetadata(
                source_id="web_2",
                source_type=SourceType.BLOG,
                url="https://example.com/blog2",
                title="Optimizing Transformer Inference",
                author="A. Johnson",
                publication_date="2023-02-20",
                retrieved_date=datetime.now().strftime("%Y-%m-%d"),
                relevance_score=0.85,
                quality_score=0.79,
                content_snippet="This blog post explains techniques for optimizing transformer inference..."
            ),
            content="Full content about transformer inference optimization...",
            keywords=["inference", "optimization", "transformers", "latency", "throughput"],
            relevance=SourceRelevance.HIGH
        ),
    }
    
    # Sample cleaned Twitter sources
    twitter_sources = {
        "twitter_1": CleanedSource(
            metadata=SourceMetadata(
                source_id="twitter_1",
                source_type=SourceType.TWITTER,
                url="https://twitter.com/user/status/123456",
                title=None,
                author="@ml_expert",
                publication_date="2023-03-05",
                retrieved_date=datetime.now().strftime("%Y-%m-%d"),
                relevance_score=0.75,
                quality_score=0.82,
                content_snippet="Just published our new paper on efficient transformer architectures..."
            ),
            content="Thread content about efficient transformer architectures...",
            keywords=["research", "transformers", "efficiency", "paper"],
            relevance=SourceRelevance.MEDIUM
        ),
    }
    
    # Sample user context
    user_context = {
        "refined_query": "Recent advances in transformer architecture efficiency improvements",
        "user_background": {
            "user_type": "Specialized Professional",
            "research_purpose": "Staying updated on AI research developments",
            "query_frequency": "weekly"
        }
    }
    
    # Create request
    request = WritingContextRequest(
        routing_id="sample_routing_id",
        cleaned_web_sources=web_sources,
        cleaned_twitter_sources=twitter_sources,
        user_context=user_context
    )
    
    try:
        print("Testing Router0_4...")
        response = await prepare_writing_context(request)
        
        print(f"Writing Context ID: {response.writing_context_id}")
        print(f"Refined Query: {response.refined_query}")
        print(f"Number of Reranked Sources: {len(response.reranked_sources)}")
        print(f"Number of Source Agents: {len(response.source_agents)}")
        print(f"Number of Thematic Clusters: {len(response.thematic_clusters)}")
        
        print("\nContext Summary:")
        print(f"  Top Topics: {response.context_summary.get('top_topics', [])}")
        print(f"  Structure Recommendation: {response.context_summary.get('writing_guidance', {}).get('structure_recommendation', '')}")
        
    except Exception as e:
        print(f"Error testing Router0_4: {e}")
        import traceback
        traceback.print_exc()
        
if __name__ == "__main__":
    import asyncio
    try:
        asyncio.run(test_router04())
    except KeyboardInterrupt:
        print("\nExiting router test.")
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()

--------------------------------------------------
File End
--------------------------------------------------


.\processes\query\refiner.py
File type: .py
"""
Query Refiner Module

This module implements the query refinement process for Vizier. It transforms raw user queries
into refined, structured research plans through an iterative LLM-guided refinement loop.

The process follows this flow:
1. User submits an initial query
2. Meta-prompt guides an LLM to generate a refined query candidate
3. User provides feedback on the refined query
4. LLM incorporates feedback to produce an improved query
5. Loop continues until user approves the refined query
6. Final approved query is returned for downstream research planning

This component is the first stage in Vizier's research workflow, ensuring high-quality
inputs for subsequent data collection, analysis, and report generation stages.
"""

import asyncio
from typing import List, Dict, Optional
from routers.openrouter import OpenRouterClient
from pydantic import BaseModel, Field

# constants
REFINER_MODEL = "openrouter/optimus-alpha"
MAX_TOKENS_REFINEMENT = 500
TEMPERATURE_REFINEMENT = 0.7

META_PROMPT = """
You are an expert research assistant AI for specialized professionals and researchers. Your task is to transform user queries into comprehensive, actionable research plans that prioritize technical accuracy, timeliness, and depth.

**User Background Information:**
- User Type: {user_type}
- Research Purpose: {research_purpose}
- User Description: {user_description}
- Query Frequency: {query_frequency}

**Goal:** Transform the user's query, which may be vague or incomplete, into a structured research plan that identifies:
1. Precise sub-topics requiring investigation
2. Specific types of sources to prioritize (e.g., academic papers, preprints, technical blogs, expert social media)
3. Key experts, institutions, or research groups to focus on
4. Temporal considerations (recent developments vs. foundational knowledge)
5. Technical depth requirements appropriate for specialized professionals

**Input:** The user will provide their current query or feedback on your previous refinement attempt. This will be the last message in the conversation history.

**Output:** Generate a single, improved version of the research query as a structured plan. You MUST return the FULL COMPLETE refined query after each iteration, not just the parts affected by feedback. Be concise but comprehensive, with clear separation between different components of the research. Focus on technical precision, avoiding oversimplification.

**Example Interaction:**

*   **User:** "Update me on CRISPR developments"
*   **You (Refined Query):**
    "Research Plan: Investigate recent technical developments in CRISPR gene-editing technology with emphasis on specialized research applications.

    Key Research Components:
    1. Latest advances in CRISPR-Cas9 and alternative systems (Cas12, Cas13) within the past 6-12 months, focusing on enhanced precision, reduced off-target effects, and delivery mechanisms.
    2. Recent (2023-present) peer-reviewed publications and preprints from leading institutions (Broad Institute, Zhang Lab, Doudna Lab, Qi Lab).
    3. Technical breakthroughs in specific application domains: therapeutic applications for genetic disorders, agricultural applications, and diagnostic tools.
    4. Regulatory and ethical developments affecting research applications, particularly recent FDA/EMA decisions.
    5. Commercial and translational progress from key biotechnology companies (Editas, CRISPR Therapeutics, Intellia, Caribou).

    Sources to prioritize: Recent journal publications in Nature Biotechnology, Cell, Science; bioRxiv/medRxiv preprints; NIH/FDA announcements; technical content from specialized conferences."

*   **User:** "Focus more on base editing applications in particular."
*   **You (Further Refined Query):**
    "Refined Research Plan: Investigate recent technical developments in base editing applications of CRISPR technology.

    Key Research Components:
    1. Technical advances in cytosine and adenine base editor (CBE/ABE) systems within the past 12 months, with particular focus on expanded targeting scope, enhanced specificity, and reduced off-target effects.
    2. Comparative analysis of latest generation base editors (BE4max, ABE8e, etc.) versus prime editing for precise genetic modifications.
    3. Recent breakthrough applications in therapeutic contexts: 
       a. Base editing approaches for monogenic disorders (Î²-thalassemia, sickle cell disease, familial hypercholesterolemia)
       b. Cancer immunotherapy applications (PD-1, TCR modifications)
       c. In vivo delivery systems optimized for base editors (LNPs, AAVs, novel vectors)
    4. Key technical limitations and engineering solutions from Liu Lab (Broad Institute), Komor Lab (UCSD), and other leading research groups.
    5. Technological convergence of base editing with other precision gene modification approaches.

    Sources to prioritize: Nature Biotechnology publications since 2023; bioRxiv preprints; scientific proceedings from American Society of Gene & Cell Therapy; recent patents; NIH CRISPR clinical trial registrations; technical publications from Beam Therapeutics and Verve Therapeutics."

**Current Task:** Based on the entire conversation history, refine the latest user input into a better research query/plan. Remember to always return the FULL COMPLETE refined query, not just address the specific feedback points. Ensure your response is structured clearly with explicit research components, specific technical details to investigate, and prioritized information sources appropriate for specialized professionals.

**Response Max Tokens:** {max_tokens}
"""


# pydantic models for API requests/responses
class UserBackground(BaseModel):
    """User background information for query refinement"""
    user_type: str = Field(description="Type of user (e.g., 'Specialized Professional')")
    research_purpose: str = Field(description="What the user will use Vizier for")
    user_description: str = Field(description="Brief description of who the user is")
    query_frequency: str = Field(description="How often the query will be run (daily/weekly/monthly)")
class QueryRequest(BaseModel):
    """Request model for query refinement"""
    query: str = Field(description="The user's query or feedback on previous refinement")
    background: UserBackground = Field(description="User background information")
class QueryResponse(BaseModel):
    """Response model for refined query"""
    refined_query: str = Field(description="The refined query")
    is_complete: bool = Field(description="Whether the refinement process is complete")
    conversation_id: Optional[str] = Field(None, description="Conversation ID for continuing the refinement")


class QueryRefiner:
    """
    Manages the iterative process of refining a user's research query using an LLM.
    """
    def __init__(self, model: str = REFINER_MODEL,
                    max_tokens: int = MAX_TOKENS_REFINEMENT,
                    temperature: float = TEMPERATURE_REFINEMENT):
        """
        Initializes the QueryRefiner.

        Args:
            model: The identifier of the language model to use for refinement.
        """
        self.client = OpenRouterClient()
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.conversation_history: List[Dict[str, str]] = []
        self._init_system_prompt(None, None, None, None, max_tokens)

    def _init_system_prompt(self, user_type: Optional[str], research_purpose: Optional[str],
                            user_description: Optional[str], query_frequency: Optional[str],
                            max_tokens: str):
        """Initialize the system prompt with user background information"""
        formatted_prompt = META_PROMPT.format(
            user_type=user_type or "Unknown",
            research_purpose=research_purpose or "Unknown",
            user_description=user_description or "Unknown",
            query_frequency=query_frequency or "Unknown",
            max_tokens=max_tokens
        )

        # clear existing history and set the formatted system prompt
        self.conversation_history = [
            {"role": "system", "content": formatted_prompt}
        ]

    def set_user_background(self, background: UserBackground):
        """
        Set user background information for the refiner.

        Args:
            background: UserBackground object with user information
        """
        self._init_system_prompt(
            background.user_type,
            background.research_purpose,
            background.user_description,
            background.query_frequency
        )


    async def refine_query(self, current_query: str) -> Optional[str]:
        """
        Performs one round of query refinement using the LLM.

        Args:
            current_query: The user's latest query or feedback.

        Returns:
            The refined query suggested by the LLM, or None if an error occurs.
        """
        # add user's latest input to conversation history
        self.conversation_history.append({"role": "user", "content": current_query})

        try: # get refined query from LLM
            response = await self.client.chat_completion(
                model=self.model,
                messages=self.conversation_history,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )

            # extract content based on OpenRouter formatting
            if isinstance(response, dict) and 'choices' in response:
                if len(response['choices']) > 0 and 'message' in response['choices'][0]:
                    refined_query = response['choices'][0]['message']['content']

                    # add the assistant's response to conversation history
                    self.conversation_history.append({"role": "assistant", "content": refined_query})
                    return refined_query.strip()

                else:
                    print("Unexpected response format - no message in choice")
                    print(f"Response data: {response}")
                    return None
            else:
                print(f"Unexpected response format: {response}")
                return None

        except Exception as e:
            print(f"Error during LLM call: {e}")
            # remove the user message that caused the error to avoid infinite loops
            if len(self.conversation_history) > 1 and self.conversation_history[-1]['role'] == 'user':
                self.conversation_history.pop()
            return None

    async def process_query(self, query: str, background: UserBackground) -> QueryResponse:
        """
        Process a new query or feedback for API integration.

        Args:
            query: User's query or feedback
            background: User background information

        Returns:
            QueryResponse with refined query and completion status
        """
        # Set up user background if this is a new conversation or background changed
        self.set_user_background(background)

        # get the refined query from the LLM
        refined_query = await self.refine_query(query)
        if refined_query is None:
            return QueryResponse(
                refined_query="Unable to process query. Please try again with a different query.",
                is_complete=False
            )

        #? since API: assume one refinement at a time, so is_complete is always False
            # frontend needs to send a new request with feedback or approval
        return QueryResponse(
            refined_query=refined_query,
            is_complete=False,
            conversation_id=str(id(self.conversation_history))  # Simple unique ID
        )

    async def finalize_query(self, conversation_id: str) -> QueryResponse:
        """
        Mark the current query as approved.

        Args:
            conversation_id: The conversation ID to finalize

        Returns:
            QueryResponse with the final query and completion status set to True
        """
        if not self.conversation_history or len(self.conversation_history) < 2:
            return QueryResponse(
                refined_query="No query has been processed yet.",
                is_complete=False
            )

        # get the last assistant message as the final query
        for message in reversed(self.conversation_history):
            if message["role"] == "assistant":
                return QueryResponse(
                    refined_query=message["content"],
                    is_complete=True,
                    conversation_id=conversation_id
                )

        return QueryResponse(
            refined_query="No refined query found in conversation history.",
            is_complete=False
        )


    #! FOR TESTING ONLY
    async def run_refinement_loop(self, initial_query: str, background: UserBackground) -> str:
        """
        Runs the interactive query refinement loop until the user approves.
        Used for CLI testing ONLY!

        Args:
            initial_query: The user's starting query.
            background: User background information.

        Returns:
            The final, user-approved refined query.
        """
        # Set up background info
        self.set_user_background(
            background.user_type,
            background.research_purpose,
            background.user_description,
            background.query_frequency
        )
        
        current_input = initial_query
        approved_query = None

        while True:
            print("\nâ³ Refining query...")
            refined_query = await self.refine_query(current_input)

            if refined_query is None:
                print("âŒ Failed to get refinement from the model. Please try again or modify your input.")
                # get the last valid input as fallback
                if len(self.conversation_history) > 1:
                    last_valid_input = current_input  # Use current input as fallback
                    print(f"Last valid input was: '{last_valid_input[:100]}...'")

                    # ask user what to do
                    retry = input("Retry using the last valid input? (y/n): ").lower()
                    if retry != 'y':
                        print("Exiting refinement loop due to error.")
                        return current_input

                    # try again with same input
                    continue
                else:
                    return initial_query

            print("\n---------------- Refined Query Suggestion ----------------")
            print(refined_query)
            print("--------------------------------------------------------")

            feedback = input("Is this refined query good? (y/n/provide feedback): ").strip()
            if feedback.lower() == 'y':
                approved_query = refined_query
                print("\nâœ… Query Approved!")
                break
            elif feedback.lower() == 'n':
                current_input = input("Please provide feedback or a new version of the query: ").strip()
                if not current_input:  # handle empty input
                    print("No feedback provided. Re-using the last suggestion for refinement.")
                    current_input = refined_query
                    # remove the last assistant message to avoid confusion
                    if self.conversation_history[-1]['role'] == 'assistant':
                        self.conversation_history.pop()
            else:
                # user provided specific feedback directly
                current_input = feedback

        await self.client.client.aclose() # close client connection
        return approved_query if approved_query else initial_query



#? FastAPI interface for query refinement
async def refine_query(request: QueryRequest) -> QueryResponse:
    """
    API endpoint for refining a user query.

    Args:
        request: The query request containing the user query and background info

    Returns:
        QueryResponse with the refined query
    """
    refiner = QueryRefiner()
    response = await refiner.process_query(request.query, request.background)
    await refiner.client.client.aclose()
    return response

async def approve_query(conversation_id: str) -> QueryResponse: #! OPTIONAL
    """
    API endpoint for approving the current refined query.

    Args:
        conversation_id: The ID of the conversation to finalize

    Returns:
        QueryResponse with the final approved query
    """
    refiner = QueryRefiner()
    response = await refiner.finalize_query(conversation_id)
    await refiner.client.client.aclose()
    return response




async def test_chat_completion(): #! FOR TESTING ONLY
    """
    Basic test function to verify OpenRouter API connectivity.
    """
    client = OpenRouterClient()
    try:
        print("Testing OpenRouter API connection...")
        response = await client.chat_completion(
            model=REFINER_MODEL,  # Using the same model as the refiner
            messages=[
                {
                    "role": "user",
                    "content": "What's the capital of France?"
                }
            ],
            max_tokens=50,
            temperature=0.7
        )
        print("Test Response:\n", response)
        print("Connection successful!")
        return True
    except Exception as e:
        print(f"Test connection failed: {e}")
        return False
    finally:
        await client.client.aclose()  # close AsyncClient


async def main(): #! FOR CLI TESTING ONLY
    """
    Main function to run the command-line query refinement tool.
    This provides a CLI testing interface that uses the same code
    as the API but with interactive prompts.
    """
    print("ðŸš€ Starting Query Refinement Process...")

    # test connection first
    connection_ok = await test_chat_completion()
    if not connection_ok:
        print("âš ï¸ Could not connect to OpenRouter API. Please check your API key and connection.")
        return

    # collect user background information
    print("\nPlease provide some background information:")
    user_type = input("User Type (e.g., 'Specialized Professional'): ").strip() or "Specialized Professional"
    research_purpose = input("What will you be using Vizier for? (200 chars max): ").strip()[:200] or "Research"
    user_description = input("Who are you? (200 chars max): ").strip()[:200] or "Researcher"

    print("\nHow frequently will you run this query?")
    print("1) Daily")
    print("2) Weekly")
    print("3) Monthly")
    frequency_choice = input("Enter number (1-3): ").strip()
    query_frequency = {
        "1": "daily",
        "2": "weekly",
        "3": "monthly"
    }.get(frequency_choice, "weekly")

    # package background info
    background = UserBackground(
        user_type=user_type,
        research_purpose=research_purpose,
        user_description=user_description,
        query_frequency=query_frequency
    )

    # get initial query
    initial_query = ""
    while not initial_query:
        print("\nTell me what you'd like to research:")
        initial_query = input("Please enter your initial research query: ").strip()
        if not initial_query:
            print("Query cannot be empty. Please try again.")

    # use the API-compatible methods for consistency
    refiner = QueryRefiner()

    # initial refinement
    response = await refiner.process_query(initial_query, background)
    final_query = response.refined_query

    # feedback-based refinement loop
    while not response.is_complete:
        print("\n---------------- Refined Query Suggestion ----------------")
        print(final_query)
        print("--------------------------------------------------------")

        feedback = input("Is this refined query good? (y/n/provide feedback): ").strip()

        if feedback.lower() == 'y':
            response = await refiner.finalize_query(response.conversation_id or "")
            break

        else: # use the feedback (or ask for it if just 'n')
            if feedback.lower() == 'n':
                feedback = input("Please provide feedback for improvement: ").strip()
                if not feedback:
                    print("No feedback provided, using previous suggestion")
                    continue

            # process the feedback and get a new refined query
            print("â³ Refining query based on feedback...")
            response = await refiner.process_query(feedback, background)
            final_query = response.refined_query

    # display final approved query
    print("\n================ Final Approved Query ================")
    print(final_query)
    print("====================================================")
    print("\nThis refined query is now ready to guide a comprehensive research process.")

    await refiner.client.client.aclose() # close client connection

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nExiting query refinement process.")
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()

--------------------------------------------------
File End
--------------------------------------------------


.\processes\report\refiner.py
File type: .py
"""
Draft Refiner Module

This module implements the draft refinement process for Vizier. It transforms initial report drafts
into polished, publication-ready documents through an iterative LLM-guided refinement loop.

The process follows this flow:
1. Initial draft is submitted
2. Meta-prompt guides an LLM to analyze and improve the draft
3. User provides feedback on the refined draft
4. LLM incorporates feedback to produce an improved version
5. Loop continues until user approves the final draft
6. Final approved draft is returned for formatting and export

This component helps ensure high-quality research report outputs by providing
systematic improvements to structure, clarity, and technical accuracy.
"""

import asyncio
from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field
from routers.openrouter import OpenRouterClient

# Constants
REFINER_MODEL = "openrouter/optimus-alpha"
MAX_TOKENS_REFINEMENT = 2000  # Higher than query refiner since handling full drafts
TEMPERATURE_REFINEMENT = 0.7

META_PROMPT = """
You are an expert research report editor focusing on technical and academic writing. Your task is to refine research report drafts through iterative improvements, maintaining high standards for clarity, structure, and technical accuracy.

**Draft Context:**
- Report Type: {report_type}
- Technical Level: {technical_level}
- Target Audience: {target_audience}
- Length Guidelines: {length_guidelines}

**Goal:** Refine the draft while focusing on:
1. Technical accuracy and precision
2. Logical flow and structure
3. Clear presentation of research findings
4. Proper integration of sources and citations
5. Balanced coverage of topics
6. Professional academic tone
7. Adherence to style guidelines

**Input:** The user will provide either an initial draft or feedback on your previous refinement. This will be the last message in the conversation history.

**Output:** Generate a single, improved version of the draft. You MUST return the COMPLETE refined draft after each iteration, not just edited sections. Focus on substantive improvements while preserving the core research content.

**Refinement Priorities:**
1. Structural Coherence:
   - Clear section transitions
   - Balanced section lengths
   - Strong topic sentences
   - Effective use of subheadings

2. Technical Content:
   - Precise technical terminology
   - Clear explanation of complex concepts
   - Proper citation of technical claims
   - Accurate representation of research findings

3. Academic Standards:
   - Formal academic tone
   - Proper citation format
   - Clear methodology description
   - Objective presentation of findings

4. Readability:
   - Clear sentence structure
   - Professional vocabulary
   - Consistent terminology
   - Effective use of technical figures/tables references

**Current Task:** Based on the conversation history, analyze the latest draft or feedback and provide a refined version. Always return the COMPLETE refined draft with all sections. Ensure your refinements maintain technical accuracy while improving clarity and structure.

**Response Max Tokens:** {max_tokens}
"""

class ReportContext(BaseModel):
    """Context information for report refinement"""
    report_type: str = Field(description="Type of report (e.g., 'Research Paper', 'Technical Report')")
    technical_level: str = Field(description="Technical depth level (e.g., 'Expert', 'Intermediate')")
    target_audience: str = Field(description="Intended audience for the report")
    length_guidelines: str = Field(description="Target length and format guidelines")

class DraftRequest(BaseModel):
    """Request model for draft refinement"""
    draft_content: str = Field(description="The current draft content or feedback")
    context: ReportContext = Field(description="Report context information")

class DraftResponse(BaseModel):
    """Response model for refined draft"""
    refined_draft: str = Field(description="The refined draft content")
    suggested_improvements: List[str] = Field(default_factory=list, description="List of suggested further improvements")
    is_complete: bool = Field(description="Whether the refinement process is complete")
    conversation_id: Optional[str] = Field(None, description="Conversation ID for continuing the refinement")

class DraftRefiner:
    """
    Manages the iterative process of refining research report drafts using an LLM.
    """
    def __init__(self, model: str = REFINER_MODEL,
                 max_tokens: int = MAX_TOKENS_REFINEMENT,
                 temperature: float = TEMPERATURE_REFINEMENT):
        """
        Initialize the DraftRefiner.

        Args:
            model: The identifier of the language model to use for refinement
            max_tokens: Maximum tokens for model response
            temperature: Temperature setting for generation
        """
        self.client = OpenRouterClient()
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.conversation_history: List[Dict[str, str]] = []
        self._init_system_prompt(None, None, None, None, max_tokens)

    def _init_system_prompt(self, report_type: Optional[str], technical_level: Optional[str],
                          target_audience: Optional[str], length_guidelines: Optional[str],
                          max_tokens: int):
        """Initialize the system prompt with report context information"""
        formatted_prompt = META_PROMPT.format(
            report_type=report_type or "Research Report",
            technical_level=technical_level or "Advanced",
            target_audience=target_audience or "Technical Professionals",
            length_guidelines=length_guidelines or "Standard research paper length",
            max_tokens=max_tokens
        )
        
        # Clear existing history and set the formatted system prompt
        self.conversation_history = [
            {"role": "system", "content": formatted_prompt}
        ]

    def set_report_context(self, context: ReportContext):
        """
        Set report context information for the refiner.

        Args:
            context: ReportContext object with report parameters
        """
        self._init_system_prompt(
            context.report_type,
            context.technical_level,
            context.target_audience,
            context.length_guidelines,
            self.max_tokens
        )

    async def refine_draft(self, current_draft: str) -> Optional[str]:
        """
        Performs one round of draft refinement using the LLM.

        Args:
            current_draft: The current draft content or feedback.

        Returns:
            The refined draft suggested by the LLM, or None if an error occurs.
        """
        # Add user's latest input to conversation history
        self.conversation_history.append({"role": "user", "content": current_draft})

        try:
            # Get refined draft from LLM
            response = await self.client.chat_completion(
                model=self.model,
                messages=self.conversation_history,
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )

            # Extract content based on OpenRouter formatting
            if isinstance(response, dict) and 'choices' in response:
                if len(response['choices']) > 0 and 'message' in response['choices'][0]:
                    refined_draft = response['choices'][0]['message']['content']
                    
                    # Add the assistant's response to conversation history
                    self.conversation_history.append({"role": "assistant", "content": refined_draft})
                    return refined_draft.strip()
                
                else:
                    print("Unexpected response format - no message in choice")
                    print(f"Response data: {response}")
                    return None
            else:
                print(f"Unexpected response format: {response}")
                return None

        except Exception as e:
            print(f"Error during LLM call: {e}")
            # Remove the user message that caused the error
            if len(self.conversation_history) > 1 and self.conversation_history[-1]['role'] == 'user':
                self.conversation_history.pop()
            return None

    async def evaluate_draft(self, draft_content: str) -> Dict[str, Any]:
        """
        Evaluate the current draft and provide specific improvement suggestions.

        Args:
            draft_content: The current draft content to evaluate

        Returns:
            Dictionary with evaluation metrics and suggested improvements
        """
        eval_prompt = f"""
        Analyze this draft and provide:
        1. Key strengths
        2. Specific areas needing improvement
        3. Technical accuracy assessment
        4. Structural coherence evaluation
        5. Writing style consistency check

        Draft content:
        {draft_content}
        """

        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": eval_prompt}],
                max_tokens=1000,
                temperature=0.5
            )

            if isinstance(response, dict) and 'choices' in response:
                evaluation = response['choices'][0]['message']['content']
                
                # Parse evaluation into structured format
                return {
                    "evaluation_text": evaluation,
                    "meets_standards": "major issues" not in evaluation.lower(),
                    "suggested_improvements": [
                        line.strip() for line in evaluation.split("\n")
                        if line.strip().startswith("-") or line.strip().startswith("*")
                    ]
                }
            
            return {
                "evaluation_text": "Error performing evaluation",
                "meets_standards": False,
                "suggested_improvements": []
            }

        except Exception as e:
            print(f"Error during draft evaluation: {e}")
            return {
                "evaluation_text": f"Evaluation failed: {str(e)}",
                "meets_standards": False,
                "suggested_improvements": []
            }

    async def process_draft(self, request: DraftRequest) -> DraftResponse:
        """
        Process a new draft or feedback for API integration.

        Args:
            request: DraftRequest with draft content and context

        Returns:
            DraftResponse with refined draft and related information
        """
        # Set up report context if this is a new conversation or context changed
        self.set_report_context(request.context)

        # Get the refined draft from the LLM
        refined_draft = await self.refine_draft(request.draft_content)
        if refined_draft is None:
            return DraftResponse(
                refined_draft="Unable to process draft. Please try again with a different draft.",
                is_complete=False,
                suggested_improvements=[]
            )

        # Evaluate the refined draft
        evaluation = await self.evaluate_draft(refined_draft)
        
        return DraftResponse(
            refined_draft=refined_draft,
            is_complete=False,  # Assume one refinement at a time for API
            suggested_improvements=evaluation.get("suggested_improvements", []),
            conversation_id=str(id(self.conversation_history))
        )

    async def finalize_draft(self, conversation_id: str) -> DraftResponse:
        """
        Mark the current draft as approved.

        Args:
            conversation_id: The conversation ID to finalize

        Returns:
            DraftResponse with the final draft and completion status set to True
        """
        if not self.conversation_history or len(self.conversation_history) < 2:
            return DraftResponse(
                refined_draft="No draft has been processed yet.",
                is_complete=False,
                suggested_improvements=[]
            )

        # Get the last assistant message as the final draft
        for message in reversed(self.conversation_history):
            if message["role"] == "assistant":
                final_evaluation = await self.evaluate_draft(message["content"])
                return DraftResponse(
                    refined_draft=message["content"],
                    is_complete=True,
                    suggested_improvements=final_evaluation.get("suggested_improvements", []),
                    conversation_id=conversation_id
                )

        return DraftResponse(
            refined_draft="No refined draft found in conversation history.",
            is_complete=False,
            suggested_improvements=[]
        )


async def refine_draft(request: DraftRequest) -> DraftResponse:
    """
    API endpoint for refining a draft.

    Args:
        request: The draft request containing the content and context

    Returns:
        DraftResponse with the refined draft
    """
    refiner = DraftRefiner()
    response = await refiner.process_draft(request)
    await refiner.client.client.aclose()
    return response

async def approve_draft(conversation_id: str) -> DraftResponse:
    """
    API endpoint for approving the current refined draft.

    Args:
        conversation_id: The ID of the conversation to finalize

    Returns:
        DraftResponse with the final approved draft
    """
    refiner = DraftRefiner()
    response = await refiner.finalize_draft(conversation_id)
    await refiner.client.client.aclose()
    return response

async def test_chat_completion():  #! FOR TESTING ONLY
    """
    Basic test function to verify OpenRouter API connectivity.
    """
    client = OpenRouterClient()
    try:
        print("Testing OpenRouter API connection...")
        response = await client.chat_completion(
            model=REFINER_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": "Briefly explain what makes a good research paper introduction."
                }
            ],
            max_tokens=100,
            temperature=0.7
        )
        print("Test Response:\n", response)
        print("Connection successful!")
        return True
    except Exception as e:
        print(f"Test connection failed: {e}")
        return False
    finally:
        await client.client.aclose()

async def main():  #! FOR CLI TESTING ONLY
    """
    Main function to run the command-line draft refinement tool.
    This provides a CLI testing interface that uses the same code
    as the API but with interactive prompts.
    """
    print("ðŸš€ Starting Draft Refinement Process...")

    # Test connection first
    connection_ok = await test_chat_completion()
    if not connection_ok:
        print("âš ï¸ Could not connect to OpenRouter API. Please check your API key and connection.")
        return

    # Collect report context information
    print("\nPlease provide report context information:")
    report_type = input("Report Type (e.g., 'Research Paper', 'Technical Report'): ").strip() or "Research Paper"
    
    print("\nSelect Technical Level:")
    print("1) Expert")
    print("2) Advanced")
    print("3) Intermediate")
    level_choice = input("Enter number (1-3): ").strip()
    technical_level = {
        "1": "Expert",
        "2": "Advanced",
        "3": "Intermediate"
    }.get(level_choice, "Advanced")

    target_audience = input("Target Audience: ").strip() or "Technical Professionals"
    length_guidelines = input("Length Guidelines (e.g., '5000 words'): ").strip() or "Standard research paper length"

    # Package context info
    context = ReportContext(
        report_type=report_type,
        technical_level=technical_level,
        target_audience=target_audience,
        length_guidelines=length_guidelines
    )

    # Get initial draft
    print("\nPlease provide your draft content.")
    print("You can paste multiple lines. When finished, press Ctrl+D (Unix) or Ctrl+Z (Windows) followed by Enter.")
    print("Begin typing/pasting your draft:")
    
    draft_lines = []
    while True:
        try:
            line = input()
            draft_lines.append(line)
        except EOFError:
            break
        except KeyboardInterrupt:
            print("\nDraft input cancelled.")
            return

    initial_draft = "\n".join(draft_lines)
    if not initial_draft.strip():
        print("Draft cannot be empty. Exiting.")
        return

    # Use the API-compatible methods for consistency
    refiner = DraftRefiner()

    # Initial refinement
    print("\nâ³ Processing initial draft...")
    response = await refiner.process_draft(DraftRequest(
        draft_content=initial_draft,
        context=context
    ))
    
    final_draft = response.refined_draft
    conversation_id = response.conversation_id

    # Feedback-based refinement loop
    while not response.is_complete:
        print("\n================ Current Draft ================")
        print(final_draft)
        print("=============================================")
        
        if response.suggested_improvements:
            print("\nSuggested Improvements:")
            for i, suggestion in enumerate(response.suggested_improvements, 1):
                print(f"{i}. {suggestion}")

        feedback = input("\nIs this draft good? (y/n/provide feedback): ").strip()

        if feedback.lower() == 'y':
            response = await refiner.finalize_draft(conversation_id or "")
            break

        else:  # Use the feedback (or ask for it if just 'n')
            if feedback.lower() == 'n':
                print("\nProvide your feedback. Press Ctrl+D (Unix) or Ctrl+Z (Windows) followed by Enter when done:")
                feedback_lines = []
                while True:
                    try:
                        line = input()
                        feedback_lines.append(line)
                    except EOFError:
                        break
                    except KeyboardInterrupt:
                        print("\nFeedback input cancelled.")
                        continue
                
                feedback = "\n".join(feedback_lines)
                if not feedback.strip():
                    print("No feedback provided, using previous version")
                    continue

            # Process the feedback and get a new refined draft 
            print("\nâ³ Refining draft based on feedback...")
            response = await refiner.process_draft(DraftRequest(
                draft_content=feedback,
                context=context
            ))
            final_draft = response.refined_draft

    # Display final approved draft
    print("\n================ Final Approved Draft ================")
    print(final_draft)
    print("====================================================")
    
    # Provide final evaluation
    print("\nFinal Draft Evaluation:")
    evaluation = await refiner.evaluate_draft(final_draft)
    if evaluation["suggested_improvements"]:
        print("\nConsider these points for future revisions:")
        for suggestion in evaluation["suggested_improvements"]:
            print(f"- {suggestion}")
    else:
        print("\nNo additional improvements suggested.")
    
    print("\nRefinement process complete! The final draft is ready for review.")
    
    await refiner.client.client.aclose()  # Close client connection

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nExiting draft refinement process.")
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()

--------------------------------------------------
File End
--------------------------------------------------


.\processes\report\writer.py
File type: .py
"""
Writer Module

This module implements the Writer agent for Vizier's report generation process. 
It transforms high-quality source data from upstream source agents into 
comprehensive research reports through an iterative refinement loop.

The process follows this flow:
1. System receives consolidated source data from Router_04
2. Writer agent synthesizes an initial comprehensive draft
3. User provides feedback on the draft report
4. Writer agent incorporates feedback to produce an improved report
5. Loop continues until user approves the final report
6. Final approved report is returned

This component follows the source exploration and filtering stages, ensuring
that only high-quality, relevant information is used in the final report.
"""

import os, asyncio, json
from typing import Dict, List, Optional, Any, Callable, TypedDict, Literal
from enum import Enum
from datetime import datetime
from pydantic import BaseModel, Field, TypeAdapter
from routers.openrouter import OpenRouterClient
from sourcing.director import SourceDirector, DirectorResponse
from sourcing.agent import SourceAgent

# constants
WRITER_MODEL = "openrouter/anthropic/claude-3-opus"
MAX_TOKENS_INTERNAL = 100000  # For internal drafts and processing
MAX_TOKENS_DRAFT = 100000     # For user-facing drafts
TEMPERATURE_DRAFT = 0.7

QUALITY_THRESHOLDS = {
    "coverage": 0.8,      # Minimum coverage of key topics
    "depth": 0.7,        # Minimum technical depth score
    "coherence": 0.75,   # Minimum coherence/flow score
    "citation": 0.9      # Minimum citation completeness
}

class ReportStyle(str, Enum):
    """Available report styles"""
    ACADEMIC = "academic"
    EXECUTIVE = "executive" 
    JOURNALISTIC = "journalistic"
    TECHNICAL = "technical"
    CONVERSATIONAL = "conversational"

class SourceReference(BaseModel):
    """Reference to a source used in the report"""
    source_id: str = Field(description="Unique identifier for the source")
    title: Optional[str] = Field(None, description="Title of the source")
    url: Optional[str] = Field(None, description="URL of the source if applicable")
    author: Optional[str] = Field(None, description="Author of the source")
    publication_date: Optional[str] = Field(None, description="Publication date of the source")
    snippet: Optional[str] = Field(None, description="Brief snippet from the source")

class SourceAgentQuery(BaseModel):
    """Query to a source agent for clarification"""
    agent_id: str = Field(description="ID of the source agent to query")
    question: str = Field(description="The question to ask the source agent")
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context for the query")

class ReportSection(BaseModel):
    """A section of the generated report"""
    title: str = Field(description="Title of the section")
    content: str = Field(description="Content of the section")
    sources: List[SourceReference] = Field(description="Sources referenced in this section")

class ReportDraft(BaseModel):
    """A draft of the research report"""
    title: str = Field(description="Title of the report")
    summary: str = Field(description="Executive summary of the report")
    sections: List[ReportSection] = Field(description="Sections of the report")
    references: List[SourceReference] = Field(description="All references used in the report")
    keywords: List[str] = Field(description="Keywords relevant to the report")

class WriterRequest(BaseModel):
    """Request model for generating a report draft"""
    writing_context_id: str = Field(description="ID from the Router_04 writing context")
    refined_query: str = Field(description="The refined research query")
    context_summary: Dict[str, Any] = Field(description="Summary context from Router_04")
    source_agents: Dict[str, Any] = Field(description="Source agents and their assignments")
    reranked_sources: Dict[str, Any] = Field(description="Reranked and filtered sources")
    thematic_clusters: Dict[str, List[str]] = Field(description="Sources grouped by theme")
    report_style: ReportStyle = Field(description="Style of report to generate")
    user_background: Dict[str, Any] = Field(description="User background information")

class WriterResponse(BaseModel):
    """Response model for a report draft"""
    draft_id: str = Field(description="Unique identifier for this draft")
    report_draft: ReportDraft = Field(description="The generated report draft")
    suggested_improvements: List[str] = Field(description="Suggestions for improving the draft")
    conversation_id: Optional[str] = Field(None, description="Conversation ID for continuing refinement")

class WriterFeedbackRequest(BaseModel):
    """Request model for providing feedback on a report draft"""
    draft_id: str = Field(description="ID of the draft to provide feedback on")
    feedback: str = Field(description="User feedback on the draft")
    conversation_id: str = Field(description="Conversation ID for the ongoing refinement")

class WriterFeedbackResponse(BaseModel):
    """Response model for feedback incorporation"""
    draft_id: str = Field(description="New draft ID")
    updated_draft: ReportDraft = Field(description="The updated report draft")
    changes_made: List[str] = Field(description="Summary of changes made based on feedback")
    is_complete: bool = Field(description="Whether the draft is considered complete")
    conversation_id: str = Field(description="Conversation ID for continuing refinement")

class QualityScore(BaseModel):
    """Quality assessment scores for a draft"""
    coverage: float = Field(description="Topic coverage score (0-1)")
    depth: float = Field(description="Technical depth score (0-1)")
    coherence: float = Field(description="Coherence and flow score (0-1)")
    citation: float = Field(description="Citation completeness score (0-1)")

class DraftEvaluation(BaseModel):
    """Evaluation results for a draft"""
    scores: QualityScore = Field(description="Quality scores")
    improvements_needed: List[str] = Field(description="Specific improvements needed")
    meets_threshold: bool = Field(description="Whether the draft meets quality thresholds")

class SourceAnalysis(BaseModel):
    """Analysis of a source's content and relevance"""
    key_information: List[Dict[str, Any]] = Field(description="Extracted key information")
    contradictions: List[Dict[str, Any]] = Field(description="Identified contradictions")
    clarification_needed: List[Dict[str, Any]] = Field(description="Questions needing clarification")
    connections: List[Dict[str, Any]] = Field(description="Thematic connections identified")

class SourceAgentMessage(TypedDict):
    """Message format for source agent communication"""
    agent_id: str
    content: str
    message_type: Literal["query", "response", "clarification"]
    context: Optional[Dict[str, Any]]

class SourceAgentConversation(BaseModel):
    """Tracks conversation with a source agent"""
    agent_id: str = Field(description="ID of the source agent")
    messages: List[SourceAgentMessage] = Field(default_factory=list, description="Conversation history")
    themes: List[str] = Field(default_factory=list, description="Themes this agent handles")
    source_types: List[str] = Field(default_factory=list, description="Types of sources this agent handles")

class WriterToSourceHandoff(BaseModel):
    """Data passed when writer hands off to a source agent"""
    query: str = Field(description="The clarification query")
    source_id: str = Field(description="ID of the source needing clarification") 
    context: Dict[str, Any] = Field(description="Additional context for the query")
    priority: int = Field(description="Query priority (1-5, 1 being highest)")

# Meta prompts for the Writer agent
WRITER_META_PROMPT = """
You are an expert research writer specializing in synthesizing complex information into clear, comprehensive reports. Your task is to create a high-quality research report based on the provided sources and user context.

# USER BACKGROUND
User Type: {user_type}
Research Purpose: {research_purpose}
Query Frequency: {query_frequency}

# RESEARCH CONTEXT
Refined Query: {refined_query}
Top Topics: {top_topics}
Writing Guidance: {writing_guidance}

# SOURCE INFORMATION
Available Sources: {source_count} sources across {source_types}
Thematic Organization: {thematic_clusters}
Source Quality Distribution: {quality_distribution}

# YOUR CAPABILITIES AND TOOLS
You have access to the following abilities to help you craft an optimal report:
1. Query sources directly to extract specific information
2. Request clarification from source agents when information is unclear or contradictory
3. Structure content according to thematic clusters or user-specified organization
4. Tailor technical depth based on user background and preferences
5. Organize information according to different structural approaches (chronological, thematic, etc.)

# YOUR RESPONSIBILITIES
1. Synthesize information from multiple high-quality sources into a coherent narrative
2. Maintain critical evaluation of source quality and reliability in your synthesis
3. Organize content logically according to identified themes and topics
4. Adapt technical depth and complexity to match user background
5. Ensure all claims are properly supported by cited sources
6. Provide balanced coverage of the topic without undue emphasis on any single source
7. Include well-structured sections with clear transitions between topics
8. Generate relevant and valuable insights based on source analysis

# REPORT STRUCTURE
Your report should include:
1. A clear title reflecting the refined query and main findings
2. An executive summary highlighting key insights and conclusions
3. Multiple well-organized sections addressing different aspects of the topic
4. Citations to specific sources throughout the report
5. A conclusion that synthesizes the findings
6. A reference list of all sources used

# PROCESS GUIDELINES
1. First, review the available sources and their thematic organization
2. Identify key topics and subtopics to cover based on source quality and relevance
3. Determine the most logical structure for the information
4. Query specific sources or agents as needed for detailed information
5. Synthesize information across sources, noting areas of consensus and disagreement
6. Draft sections that build toward comprehensive understanding
7. Ensure proper citation of sources throughout

Your goal is to produce a research report that is accurate, comprehensive, well-organized, and tailored to the user's needs and background.

# REPORT STYLE
Style Guidance: {report_style}

Response Format: Generate a complete report draft structured according to the ReportDraft model specification. Each section should include relevant source citations.
"""

EVALUATION_PROMPT = """
You are evaluating a research report draft for quality and completeness. Your task is to assess:

1. Topic Coverage (0-1):
   - Are all key topics from the research plan addressed?
   - Is the coverage balanced and thorough?

2. Technical Depth (0-1):
   - Does the analysis match the user's expertise level?
   - Are technical concepts explained appropriately?

3. Coherence & Flow (0-1):
   - Does the narrative flow logically?
   - Are sections well-connected?

4. Citation Completeness (0-1):
   - Are claims properly supported by sources?
   - Are high-quality sources utilized effectively?

Return a JSON object with scores and specific improvement needs:
{
    "scores": {
        "coverage": float,
        "depth": float,
        "coherence": float,
        "citation": float
    },
    "improvements_needed": [
        "specific improvement suggestion",
        ...
    ],
    "meets_threshold": bool
}
"""

SOURCE_EXPLORATION_PROMPT = """
You are analyzing source material to extract key information for a research report.

Source Context:
{source_context}

Current Knowledge Gaps:
{knowledge_gaps}

Your task is to:
1. Identify the most relevant information from this source
2. Note any contradictions with other sources
3. Flag areas needing clarification from source agents
4. Suggest connections to other topics/themes

Return a JSON object with your analysis:
{
    "key_information": [
        {"content": "extracted info", "relevance": float, "theme": "theme"}
    ],
    "contradictions": [
        {"content": "contradiction details", "source_ids": ["id1", "id2"]}
    ],
    "clarification_needed": [
        {"question": "specific question", "agent_id": "agent_id"}
    ],
    "connections": [
        {"from_theme": "current theme", "to_theme": "related theme", "relationship": "explanation"}
    ]
}
"""

class Writer:
    """Empowered writer agent with source agent coordination"""
    
    def __init__(self, model: str = WRITER_MODEL):
        self.client = OpenRouterClient()
        self.model = model
        self.max_tokens = MAX_TOKENS_DRAFT
        self.temperature = TEMPERATURE_DRAFT
        self.conversation_history = []
        self.context_summary = None
        self.source_agents = {}
        self.reranked_sources = {}
        self.thematic_clusters = {}
        self.user_background = {}
        self.draft_iterations = []
        self.source_analyses = {}
        self.agent_clarifications = {}
        self.knowledge_gaps = []
        self.agent_conversations: Dict[str, SourceAgentConversation] = {}
        self.reasoning_log: List[Dict[str, Any]] = []
        self.source_director = SourceDirector()
        self.source_agent_states: Dict[str, Dict[str, Any]] = {}
        self.pending_clarifications: List[WriterToSourceHandoff] = []

    def _generate_draft_id(self) -> str:
        """Generate a unique identifier for a report draft"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        random_part = os.urandom(4).hex()
        return f"draft_{timestamp}_{random_part}"
        
    def _init_system_prompt(
        self,
        context_summary: Dict[str, Any],
        source_agents: Dict[str, Any],
        report_style: ReportStyle,
        user_background: Dict[str, Any]
    ):
        """
        Initialize the system prompt with context from Router_04 and user background.
        
        Args:
            context_summary: Summary context from Router_04
            source_agents: Information on source agents and their assignments
            report_style: Style preference for the report
            user_background: User background information
        """
        # Extract relevant fields for the prompt
        refined_query = context_summary.get("refined_query", "")
        top_topics = context_summary.get("top_topics", [])
        writing_guidance = context_summary.get("writing_guidance", {})
        
        # Source metadata
        source_count = len(context_summary.get("sources", {}))
        source_types = context_summary.get("source_composition", {})
        thematic_clusters = context_summary.get("thematic_clusters", {})
        
        # Quality information for sources
        qualities = [s.get("quality_score", 0) for s in context_summary.get("sources", {}).values()]
        quality_distribution = {
            "high": sum(1 for q in qualities if q >= 0.8),
            "medium": sum(1 for q in qualities if 0.5 <= q < 0.8),
            "low": sum(1 for q in qualities if q < 0.5)
        }
        
        # User background information
        user_type = user_background.get("user_type", "Researcher")
        research_purpose = user_background.get("research_purpose", "Research")
        query_frequency = user_background.get("query_frequency", "occasional")
        
        # Format the meta prompt
        formatted_prompt = WRITER_META_PROMPT.format(
            user_type=user_type,
            research_purpose=research_purpose,
            query_frequency=query_frequency,
            refined_query=refined_query,
            top_topics=", ".join(top_topics[:5]),
            writing_guidance=json.dumps(writing_guidance, indent=2),
            source_count=source_count,
            source_types=", ".join([f"{k}: {v}" for k, v in source_types.items()]),
            thematic_clusters=len(thematic_clusters),
            quality_distribution=json.dumps(quality_distribution),
            report_style=report_style.value
        )
        
        # Clear existing history and set the formatted system prompt
        self.conversation_history = [
            {"role": "system", "content": formatted_prompt}
        ]
    
    async def _analyze_source(self, source_id: str, content: str) -> SourceAnalysis:
        """Analyze a source to extract key information and identify gaps"""
        prompt = SOURCE_EXPLORATION_PROMPT.format(
            source_context=content,
            knowledge_gaps=json.dumps(self.knowledge_gaps)
        )
        
        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=MAX_TOKENS_INTERNAL,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            analysis = json.loads(response["choices"][0]["message"]["content"])
            return SourceAnalysis(**analysis)
            
        except Exception as e:
            print(f"Error analyzing source {source_id}: {e}")
            return SourceAnalysis(
                key_information=[],
                contradictions=[],
                clarification_needed=[],
                connections=[]
            )
            
    async def _evaluate_draft(self, draft: ReportDraft) -> DraftEvaluation:
        """Evaluate a draft for quality and completeness"""
        eval_context = {
            "draft": draft.dict(),
            "user_background": self.user_background,
            "thematic_clusters": self.thematic_clusters
        }
        
        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[
                    {"role": "system", "content": EVALUATION_PROMPT},
                    {"role": "user", "content": json.dumps(eval_context)}
                ],
                max_tokens=MAX_TOKENS_INTERNAL,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            evaluation = json.loads(response["choices"][0]["message"]["content"])
            return DraftEvaluation(**evaluation)
            
        except Exception as e:
            print(f"Error evaluating draft: {e}")
            return DraftEvaluation(
                scores=QualityScore(coverage=0, depth=0, coherence=0, citation=0),
                improvements_needed=["Error in evaluation"],
                meets_threshold=False
            )

    async def _explore_sources(self) -> None:
        """Autonomously explore sources and identify areas needing clarification"""
        analyses = await asyncio.gather(*[
            self._analyze_source(source_id, source.content)
            for source_id, source in self.reranked_sources.items()
        ])
        
        for source_id, analysis in zip(self.reranked_sources.keys(), analyses):
            self.source_analyses[source_id] = analysis
            
        for source_id, analysis in self.source_analyses.items():
            for clarification in analysis.clarification_needed:
                if clarification["agent_id"] not in self.agent_clarifications:
                    self.agent_clarifications[clarification["agent_id"]] = []
                self.agent_clarifications[clarification["agent_id"]].append({
                    "source_id": source_id,
                    "question": clarification["question"]
                })
                
        self.knowledge_gaps = [
            gap for analysis in analyses
            for gap in analysis.clarification_needed
        ]

    async def _coordinate_source_agents(self, clarification_requests: List[WriterToSourceHandoff]) -> None:
        """Coordinate multiple source agents for parallel clarifications"""
        
        # Group requests by source agent
        agent_requests: Dict[str, List[WriterToSourceHandoff]] = {}
        for request in clarification_requests:
            agent_id = next(
                (aid for aid, state in self.source_agent_states.items() 
                 if request.source_id in state.get("assigned_sources", [])),
                None
            )
            if agent_id:
                if agent_id not in agent_requests:
                    agent_requests[agent_id] = []
                agent_requests[agent_id].append(request)

        # Process requests in parallel
        async def process_agent_requests(agent_id: str, requests: List[WriterToSourceHandoff]):
            try:
                for request in sorted(requests, key=lambda r: r.priority):
                    clarification = await self.source_director.get_clarification({
                        "agent_id": agent_id,
                        "source_id": request.source_id,
                        "query": request.query,
                        "context": request.context
                    })
                    
                    # Store clarification result
                    if request.source_id not in self.source_analyses:
                        self.source_analyses[request.source_id] = {}
                    self.source_analyses[request.source_id][request.query] = clarification
                    
            except Exception as e:
                print(f"Error processing requests for agent {agent_id}: {e}")

        # Run all agent requests in parallel
        await asyncio.gather(*[
            process_agent_requests(agent_id, requests)
            for agent_id, requests in agent_requests.items()
        ])

    async def _get_agent_clarification(self, handoff: WriterToSourceHandoff) -> Optional[str]:
        """Get clarification from a specific source agent with proper handoff"""
        try:
            # Find the responsible agent
            agent_id = next(
                (aid for aid, state in self.source_agent_states.items() 
                 if handoff.source_id in state.get("assigned_sources", [])),
                None
            )
            
            if not agent_id:
                return None
                
            # Request clarification through director
            response = await self.source_director.get_clarification({
                "agent_id": agent_id,
                "source_id": handoff.source_id,
                "query": handoff.query,
                "context": handoff.context
            })
            
            return response.clarification if response else None
            
        except Exception as e:
            print(f"Error getting clarification: {e}")
            return None

    async def generate_draft(self, request: WriterRequest) -> WriterResponse:
        """Generate an optimal draft through autonomous exploration and iteration"""
        # Initialize with context
        self._init_system_prompt(
            request.context_summary,
            request.source_agents,
            request.report_style,
            request.user_background
        )
        
        # Store context and initialize source agent states
        self.context_summary = request.context_summary
        self.source_agents = request.source_agents
        self.reranked_sources = request.reranked_sources
        self.thematic_clusters = request.thematic_clusters
        self.user_background = request.user_background
        
        # Initialize source agent states from context
        self.source_agent_states = {
            agent_id: {
                "assigned_sources": info.get("assigned_sources", []),
                "themes": info.get("themes", []),
                "priority": info.get("priority", 99)
            }
            for agent_id, info in request.source_agents.items()
        }
        
        # Phase 1: Autonomous Source Exploration
        self._log_reasoning(
            "source_exploration",
            {"source_count": len(request.reranked_sources)},
            "Analyzing sources to extract key information and identify knowledge gaps"
        )
        await self._explore_sources()
        
        # Phase 2: Get Clarifications from Source Agents (updated with coordination)
        if self.knowledge_gaps:
            self._log_reasoning(
                "request_clarifications",
                {"gaps": self.knowledge_gaps},
                "Requesting coordinated clarifications from source agents"
            )
            
            # Convert knowledge gaps to handoff requests
            clarification_requests = []
            for gap in self.knowledge_gaps:
                if isinstance(gap, dict) and "agent_id" in gap and "question" in gap:
                    handoff = WriterToSourceHandoff(
                        query=gap["question"],
                        source_id=gap.get("source_id", "unknown"),
                        context={"gap_type": gap.get("type", "general")},
                        priority=gap.get("priority", 3)
                    )
                    clarification_requests.append(handoff)
            
            # Process all clarifications in parallel
            await self._coordinate_source_agents(clarification_requests)

        # Phase 3: Generate Initial Draft
        try:
            # Format the core writing prompt
            prompt = f"""Based on our exploration of the sources and gathered clarifications, 
            generate a comprehensive research report that addresses:

            1. Key findings from source analysis
            2. Integration of clarified points from source agents
            3. Thematic organization based on identified clusters
            4. Technical depth appropriate for the user's background

            Context Summary: {json.dumps(self.context_summary)}
            Source Analyses: {json.dumps(self.source_analyses)}
            Agent Clarifications: {json.dumps(self.agent_clarifications)}
            """

            # Get initial draft from LLM
            response = await self.client.chat_completion(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.conversation_history[0]["content"]},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS_DRAFT,
                temperature=0.7
            )
            
            initial_content = response["choices"][0]["message"]["content"]
            current_draft = self._parse_draft_content(initial_content, self.context_summary.get("refined_query", ""))
            
        except Exception as e:
            print(f"Error generating initial draft: {e}")
            current_draft = ReportDraft(
                title=self.context_summary.get("refined_query", "Research Report"),
                summary="Error generating initial draft. Please try again.",
                sections=[],
                references=[],
                keywords=[]
            )
        
        # Phase 4: Autonomous Draft Iteration
        max_iterations = 5
        iteration = 0
        evaluation = None
        
        while iteration < max_iterations:
            evaluation = await self._evaluate_draft(current_draft)
            
            if evaluation.meets_threshold:
                break
                
            current_draft = await self._iterate_draft(current_draft)
            iteration += 1
            
        # Return the final draft with iteration history
        return WriterResponse(
            draft_id=self._generate_draft_id(),
            report_draft=current_draft,
            suggested_improvements=evaluation.improvements_needed if evaluation and not evaluation.meets_threshold else [],
            conversation_id=str(id(self.conversation_history))
        )

async def generate_draft(request: WriterRequest) -> WriterResponse:
    writer = Writer()
    response = await writer.generate_draft(request)
    await writer.client.client.aclose()
    return response

async def refine_draft(request: WriterFeedbackRequest) -> WriterFeedbackResponse:
    writer = Writer()
    response = await writer.refine_draft(request)
    await writer.client.client.aclose()
    return response

--------------------------------------------------
File End
--------------------------------------------------


.\processes\search\agents.py
File type: .py
"""
Source Search Agents Module

This module implements the web and social media search agents that fetch and process 
sources based on the router's guidance. It includes:

1. Web search agent using a combination of search APIs
2. Twitter/X search agent for social media content
3. Source reranking logic to combine and prioritize results
"""

import asyncio
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from routers.openrouter import OpenRouterClient

# Agent model configs
WEB_AGENT_MODEL = "openrouter/anthropic/claude-3-sonnet:online"  # Uses OpenRouter's web search
TWITTER_AGENT_MODEL = "openrouter/mistralai/mixtral-8x7b-instruct" # For now, we simulate Twitter search
RERANK_MODEL = "openrouter/anthropic/claude-3-sonnet"  # Good at understanding relevance and quality

class SearchResult(BaseModel):
    """A single search result from any source"""
    id: str
    title: str
    url: str
    content: str
    source_type: str  # "web" or "twitter"
    metadata: Dict[str, Any]
    relevance_score: Optional[float] = None

async def execute_web_search(context: Dict[str, Any]) -> List[SearchResult]:
    """Execute web search based on router-provided context"""
    client = OpenRouterClient()
    try:
        # The :online model variant will automatically do web search
        response = await client.chat_completion(
            model=WEB_AGENT_MODEL,
            messages=[{
                "role": "system",
                "content": f"You are an expert web researcher focusing on {context.get('domain', 'general')} content."
            }, {
                "role": "user",
                "content": f"Search query: {context.get('search_query')}.\nConstraints: {context.get('constraints')}"
            }],
            max_tokens=2000
        )
        
        # Parse results from Claude's web search response
        results = []
        # ... parse response into SearchResult objects ...
        
        return results
        
    finally:
        await client.client.aclose()

async def execute_twitter_search(context: Dict[str, Any]) -> List[SearchResult]:
    """Execute Twitter/X search based on router-provided context"""
    client = OpenRouterClient()
    try:
        # For now we'll simulate Twitter search through LLM
        # In production this would use the Twitter API
        response = await client.chat_completion(
            model=TWITTER_AGENT_MODEL,
            messages=[{
                "role": "system",
                "content": "You are an expert at finding relevant Twitter threads and discussions."
            }, {
                "role": "user", 
                "content": f"Find Twitter discussions about: {context.get('search_query')}"
            }],
            max_tokens=1000
        )
        
        results = []
        # ... parse response into SearchResult objects ...
        
        return results
        
    finally:
        await client.client.aclose()

async def rerank_sources(sources: List[SearchResult]) -> List[SearchResult]:
    """Rerank combined sources based on relevance and quality"""
    client = OpenRouterClient()
    try:
        # Have Claude analyze and score each source
        source_texts = [f"Title: {s.title}\nContent: {s.content[:500]}..." for s in sources]
        
        response = await client.chat_completion(
            model=RERANK_MODEL,
            messages=[{
                "role": "system",
                "content": "You are an expert at evaluating source quality and relevance."
            }, {
                "role": "user",
                "content": f"Score these sources from 0-10 based on relevance and quality:\n\n" + "\n---\n".join(source_texts)
            }],
            max_tokens=1000
        )
        
        # Parse scores and sort
        # ... assign scores to sources ...
        sources.sort(key=lambda x: x.relevance_score or 0, reverse=True)
        
        return sources
        
    finally:
        await client.client.aclose()

--------------------------------------------------
File End
--------------------------------------------------


.\processes\search\twitter.py
File type: .py
"""Twitter Search Agent Module

This module implements an autonomous Twitter search agent that:
1. Discovers relevant expert profiles through web search
2. Constructs advanced Twitter queries
3. Handles thread expansion and analysis
4. Validates author credibility
5. Merges and ranks results
"""

from typing import Dict, List, Optional, Any, Set
from datetime import datetime, timedelta
from pydantic import BaseModel, Field
import httpx
import traceback

# Constants
MAX_EXPERTS_TO_FIND = 25
MIN_EXPERT_SCORE = 0.7
MAX_TWEETS_PER_EXPERT = 50
DEFAULT_TIMEOUT = 30.0

class ExpertProfile(BaseModel):
    """A discovered expert profile"""
    username: str = Field(description="Twitter username")
    display_name: str = Field(description="Display name")
    bio: Optional[str] = Field(None, description="Profile bio")
    followers: Optional[int] = Field(None, description="Follower count")
    expertise_areas: List[str] = Field(description="Areas of expertise")
    credibility_score: float = Field(description="Computed credibility score")
    discovery_source: str = Field(description="How the expert was found")
    validation_signals: Dict[str, Any] = Field(description="Credibility signals")

class Tweet(BaseModel):
    """A processed tweet with metadata"""
    tweet_id: str = Field(description="Tweet ID")
    text: str = Field(description="Tweet text")
    author: str = Field(description="Author username")
    created_at: str = Field(description="Creation timestamp")
    retweet_count: Optional[int] = Field(None, description="Number of retweets")
    like_count: Optional[int] = Field(None, description="Number of likes")
    reply_count: Optional[int] = Field(None, description="Number of replies")
    quote_count: Optional[int] = Field(None, description="Number of quotes")
    is_retweet: bool = Field(description="Whether this is a retweet")
    is_quote: bool = Field(description="Whether this is a quote tweet")
    in_thread: bool = Field(description="Whether part of a thread")
    thread_id: Optional[str] = Field(None, description="ID of parent thread")
    urls: List[str] = Field(description="URLs mentioned")
    mentions: List[str] = Field(description="Users mentioned")
    hashtags: List[str] = Field(description="Hashtags used")

class Thread(BaseModel):
    """A Twitter thread"""
    thread_id: str = Field(description="Thread ID")
    author: str = Field(description="Thread author")
    tweets: List[Tweet] = Field(description="Tweets in thread")
    total_engagement: Dict[str, int] = Field(description="Aggregated engagement")
    summary: str = Field(description="Thread summary")
    key_points: List[str] = Field(description="Key points from thread")
    relevance_score: float = Field(description="Relevance to query")

class SearchQuery(BaseModel):
    """A Twitter advanced search query"""
    base_query: str = Field(description="Core search terms")
    authors: Optional[List[str]] = Field(None, description="Authors to include")
    exclude_authors: Optional[List[str]] = Field(None, description="Authors to exclude")
    min_replies: Optional[int] = Field(None, description="Minimum replies")
    min_retweets: Optional[int] = Field(None, description="Minimum retweets")
    min_likes: Optional[int] = Field(None, description="Minimum likes")
    since: Optional[str] = Field(None, description="Start date")
    until: Optional[str] = Field(None, description="End date")
    lang: str = Field(default="en", description="Language")
    filters: List[str] = Field(default_list=[], description="Additional filters")

class TwitterAgent:
    """
    Autonomous Twitter search agent that discovers experts and relevant content.
    
    The agent can:
    1. Use web search to find relevant Twitter experts
    2. Validate expert credibility through multiple signals
    3. Construct advanced search queries
    4. Process and expand threads
    5. Rank and filter results
    """
    
    def __init__(
        self,
        twitter_api_key: str,
        context: Dict[str, Any],
        web_search_key: Optional[str] = None
    ):
        """
        Initialize the Twitter agent.
        
        Args:
            twitter_api_key: Twitter API key for authentication
            context: Search context including query and user info
            web_search_key: Optional API key for web search (expert discovery)
        """
        self.twitter_api_key = twitter_api_key
        self.web_search_key = web_search_key
        self.context = context
        self.client = httpx.AsyncClient(timeout=DEFAULT_TIMEOUT)
        
        # Load Twitter search documentation
        with open("docs/twitter/SEARCH.md", "r") as f:
            self.search_docs = f.read()
            
        # Internal state
        self.discovered_experts: Dict[str, ExpertProfile] = {}
        self.found_threads: Dict[str, Thread] = {}
        self.found_tweets: Dict[str, Tweet] = {}
        self.seen_tweet_ids: Set[str] = set()
        self.quality_thresholds = {
            "min_expert_score": MIN_EXPERT_SCORE,
            "min_engagement": {
                "replies": 2,
                "retweets": 5,
                "likes": 10
            }
        }

    async def discover_experts(self, query: str) -> List[ExpertProfile]:
        """
        Use web search to discover relevant Twitter experts.
        
        Strategy:
        1. Search for "[topic] experts twitter"
        2. Look for Twitter lists, collection pages
        3. Find frequently cited experts
        4. Validate discovered profiles
        """
        experts = []
        
        try:
            # First try finding curated lists/collections
            list_results = await self._search_twitter_lists(query)
            for result in list_results:
                usernames = self._extract_usernames(result)
                for username in usernames:
                    if username not in self.discovered_experts:
                        profile = await self._validate_expert(username, query)
                        if profile and profile.credibility_score >= MIN_EXPERT_SCORE:
                            experts.append(profile)
                            self.discovered_experts[username] = profile
                            
            # Then try direct web search for experts
            search_results = await self._search_web_for_experts(query)
            for result in search_results:
                usernames = self._extract_usernames(result)
                for username in usernames:
                    if username not in self.discovered_experts:
                        profile = await self._validate_expert(username, query)
                        if profile and profile.credibility_score >= MIN_EXPERT_SCORE:
                            experts.append(profile)
                            self.discovered_experts[username] = profile
                            
        except Exception as e:
            print(f"Error discovering experts: {e}")
            traceback.print_exc()
            
        return experts

    async def _validate_expert(
        self,
        username: str,
        topic: str
    ) -> Optional[ExpertProfile]:
        """
        Validate a potential expert through multiple signals.
        
        Checks:
        1. Profile metrics (followers, following ratio)
        2. Bio relevance to topic
        3. Tweet history analysis
        4. External validation (mentions, citations)
        """
        try:
            # Get profile info
            profile = await self._get_twitter_profile(username)
            if not profile:
                return None
                
            # Basic metrics check
            if not self._check_basic_metrics(profile):
                return None
                
            # Analyze bio and tweets
            relevance_score = self._analyze_profile_relevance(profile, topic)
            engagement_score = await self._analyze_tweet_engagement(username)
            citation_score = await self._check_external_citations(username, topic)
            
            # Calculate overall score
            credibility_score = (
                relevance_score * 0.4 +
                engagement_score * 0.3 +
                citation_score * 0.3
            )
            
            if credibility_score < MIN_EXPERT_SCORE:
                return None
                
            # Create expert profile
            return ExpertProfile(
                username=username,
                display_name=profile.get("name", ""),
                bio=profile.get("description", ""),
                followers=profile.get("followers_count", 0),
                expertise_areas=self._extract_expertise_areas(profile, topic),
                credibility_score=credibility_score,
                discovery_source="profile_analysis",
                validation_signals={
                    "relevance_score": relevance_score,
                    "engagement_score": engagement_score,
                    "citation_score": citation_score,
                    "followers": profile.get("followers_count", 0),
                    "verified": profile.get("verified", False)
                }
            )
            
        except Exception as e:
            print(f"Error validating expert {username}: {e}")
            return None

    async def construct_search_queries(
        self,
        base_query: str,
        experts: List[ExpertProfile]
    ) -> List[SearchQuery]:
        """
        Construct multiple search queries based on discovered experts.
        
        Strategies:
        1. Expert-specific queries
        2. Topic-focused queries
        3. Time-based queries
        4. Engagement-filtered queries
        """
        queries = []
        
        # Base time range
        since_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")
        
        # Expert-specific queries
        for expert in experts[:MAX_EXPERTS_TO_FIND]:
            queries.append(SearchQuery(
                base_query=base_query,
                authors=[expert.username],
                min_replies=self.quality_thresholds["min_engagement"]["replies"],
                since=since_date,
                lang="en",
                filters=["filter:has_engagement", "-filter:retweets"]
            ))
            
        # Topic queries with engagement filters
        queries.append(SearchQuery(
            base_query=f"{base_query} filter:links",
            min_retweets=25,
            min_likes=100,
            since=since_date,
            lang="en",
            filters=["filter:has_engagement", "-filter:retweets"]
        ))
        
        # Recent developments query
        recent_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        queries.append(SearchQuery(
            base_query=f"{base_query} latest developments",
            since=recent_date,
            lang="en",
            filters=["filter:has_engagement", "-filter:retweets"]
        ))
        
        return queries

    async def execute_search_query(self, query: SearchQuery) -> List[Tweet]:
        """Execute a single search query and process results"""
        try:
            # Construct advanced search query
            search_query = self._build_advanced_query(query)
            
            # Execute search
            results = await self._search_twitter(search_query)
            
            # Process tweets
            tweets = []
            for result in results:
                tweet = self._process_tweet(result)
                if tweet and tweet.tweet_id not in self.seen_tweet_ids:
                    tweets.append(tweet)
                    self.seen_tweet_ids.add(tweet.tweet_id)
                    
                    # If it's a thread, expand it
                    if tweet.in_thread:
                        thread = await self._expand_thread(tweet)
                        if thread:
                            self.found_threads[thread.thread_id] = thread
                            
            return tweets
            
        except Exception as e:
            print(f"Error executing search query: {e}")
            traceback.print_exc()
            return []

    def _build_advanced_query(self, query: SearchQuery) -> str:
        """
        Build an advanced Twitter search query string.
        Uses the syntax documented in Twitter advanced search guide.
        """
        parts = [query.base_query]
        
        if query.authors:
            parts.extend(f"from:{author}" for author in query.authors)
            
        if query.exclude_authors:
            parts.extend(f"-from:{author}" for author in query.exclude_authors)
            
        if query.min_replies:
            parts.append(f"min_replies:{query.min_replies}")
            
        if query.min_retweets:
            parts.append(f"min_retweets:{query.min_retweets}")
            
        if query.min_likes:
            parts.append(f"min_faves:{query.min_likes}")
            
        if query.since:
            parts.append(f"since:{query.since}")
            
        if query.until:
            parts.append(f"until:{query.until}")
            
        if query.lang:
            parts.append(f"lang:{query.lang}")
            
        parts.extend(query.filters)
        
        return " ".join(parts)

    async def _expand_thread(self, tweet: Tweet) -> Optional[Thread]:
        """
        Expand a tweet thread by fetching all related tweets.
        
        Args:
            tweet: The tweet that's part of a thread
            
        Returns:
            Thread object if successful
        """
        try:
            # Get conversation ID
            conv_id = tweet.thread_id or tweet.tweet_id
            
            # Fetch conversation
            results = await self._get_conversation(conv_id)
            
            # Process tweets
            tweets = []
            total_engagement = {
                "replies": 0,
                "retweets": 0,
                "likes": 0,
                "quotes": 0
            }
            
            for result in results:
                thread_tweet = self._process_tweet(result)
                if thread_tweet:
                    tweets.append(thread_tweet)
                    # Update engagement
                    total_engagement["replies"] += thread_tweet.reply_count or 0
                    total_engagement["retweets"] += thread_tweet.retweet_count or 0
                    total_engagement["likes"] += thread_tweet.like_count or 0
                    total_engagement["quotes"] += thread_tweet.quote_count or 0
                    
            if tweets:
                # Create thread
                thread = Thread(
                    thread_id=conv_id,
                    author=tweets[0].author,
                    tweets=sorted(tweets, key=lambda t: t.created_at),
                    total_engagement=total_engagement,
                    summary=self._summarize_thread(tweets),
                    key_points=self._extract_thread_key_points(tweets),
                    relevance_score=self._calculate_thread_relevance(
                        tweets,
                        self.context.get("refined_query", "")
                    )
                )
                return thread
                
        except Exception as e:
            print(f"Error expanding thread {tweet.tweet_id}: {e}")
            
        return None

    def _summarize_thread(self, tweets: List[Tweet]) -> str:
        """Generate a concise summary of a thread"""
        # TODO: Implement more sophisticated summarization
        # For now, return first tweet text
        return tweets[0].text if tweets else ""

    def _extract_thread_key_points(self, tweets: List[Tweet]) -> List[str]:
        """Extract key points from a thread"""
        # TODO: Implement more sophisticated key point extraction
        # For now, return first 3 tweets
        return [t.text for t in tweets[:3]]

    def _calculate_thread_relevance(
        self,
        tweets: List[Tweet],
        query: str
    ) -> float:
        """Calculate thread relevance to query"""
        # TODO: Implement more sophisticated relevance calculation
        # For now, return 0.8 if query terms appear in first tweet
        query_terms = set(query.lower().split())
        first_tweet = tweets[0].text.lower() if tweets else ""
        matches = sum(1 for term in query_terms if term in first_tweet)
        return min(1.0, matches / len(query_terms)) if query_terms else 0.0

    async def execute_search_strategy(self) -> Dict[str, Any]:
        """
        Execute the full Twitter search strategy.
        
        Steps:
        1. Discover relevant experts
        2. Construct search queries
        3. Execute searches
        4. Process and expand threads
        5. Rank results
        
        Returns:
            Dict containing experts, threads, and individual tweets
        """
        stats = {"started": datetime.now().isoformat()}
        
        try:
            # Get refined query
            query = self.context.get("refined_query", "")
            
            # Discover experts first
            experts = await self.discover_experts(query)
            stats["experts_found"] = len(experts)
            
            # Construct search queries
            queries = await self.construct_search_queries(query, experts)
            stats["queries_planned"] = len(queries)
            
            # Execute all queries
            all_tweets = []
            for q in queries:
                tweets = await self.execute_search_query(q)
                all_tweets.extend(tweets)
                
            stats["tweets_found"] = len(all_tweets)
            stats["threads_found"] = len(self.found_threads)
            
            # Store non-thread tweets
            for tweet in all_tweets:
                if not tweet.in_thread:
                    self.found_tweets[tweet.tweet_id] = tweet
                    
            stats["completed"] = datetime.now().isoformat()
            
            return {
                "experts": self.discovered_experts,
                "threads": self.found_threads,
                "tweets": self.found_tweets,
                "statistics": stats
            }
            
        except Exception as e:
            print(f"Error in search strategy: {e}")
            traceback.print_exc()
            stats["error"] = str(e)
            stats["completed"] = datetime.now().isoformat()
            return {
                "experts": {},
                "threads": {},
                "tweets": {},
                "statistics": stats
            }

--------------------------------------------------
File End
--------------------------------------------------


.\processes\search\web.py
File type: .py
"""Web Search Agent Module

This module implements an autonomous web search agent that:
1. Plans and executes Perplexica searches
2. Validates sources and extracts metadata
3. Ranks and filters results
4. Handles academic and technical content discovery
"""

from typing import Dict, List, Optional, Any, Set
from datetime import datetime
from pydantic import BaseModel, Field
import httpx
from urllib.parse import urlparse
import traceback

# Constants
DEFAULT_TIMEOUT = 30.0
MIN_QUALITY_SCORE = 0.6
MAX_DEPTH = 3  # Max depth for following related links

class WebSource(BaseModel):
    """A processed web source with metadata"""
    url: str = Field(description="Source URL")
    title: str = Field(description="Page title")
    author: Optional[str] = Field(None, description="Author if available")
    date_published: Optional[str] = Field(None, description="Publication date")
    date_retrieved: str = Field(description="Retrieval timestamp")
    content_type: str = Field(description="Content type (article, paper, etc)")
    domain: str = Field(description="Source domain")
    content_snippet: str = Field(description="Brief content preview")
    keywords: List[str] = Field(description="Extracted keywords")
    quality_signals: Dict[str, Any] = Field(description="Quality indicators")
    credibility_score: float = Field(description="Computed credibility")
    relevance_score: float = Field(description="Relevance to query")
    depth: int = Field(description="Discovery depth (0 = direct result)")

class SearchIntent(BaseModel):
    """Search intent and parameters"""
    query: str = Field(description="Core search query")
    focus: str = Field(description="Search focus (academic/technical/news)")
    time_range: Optional[str] = Field(None, description="Time range constraint")
    required_terms: List[str] = Field(description="Must-include terms")
    excluded_terms: List[str] = Field(description="Must-exclude terms")
    domain_filters: Optional[List[str]] = Field(None, description="Allowed domains")

class PerplexicaConfig(BaseModel):
    """Perplexica API configuration"""
    chat_model: Dict[str, str] = Field(description="Chat model settings")
    embedding_model: Dict[str, str] = Field(description="Embedding model settings")
    optimization_mode: str = Field(description="Speed vs quality tradeoff")
    focus_mode: str = Field(description="Search focus mode")
    stream: bool = Field(description="Whether to stream results")

class WebAgent:
    """
    Autonomous web search agent powered by Perplexica.
    
    Features:
    1. Intelligent query planning and execution
    2. Source validation and credibility scoring
    3. Content type detection and processing
    4. Related content discovery
    5. Academic and technical focus modes
    """
    
    def __init__(
        self,
        perplexica_api_key: str,
        context: Dict[str, Any]
    ):
        """
        Initialize the web search agent.
        
        Args:
            perplexica_api_key: Perplexica API key
            context: Search context with query and user info
        """
        self.api_key = perplexica_api_key
        self.context = context
        self.client = httpx.AsyncClient(timeout=DEFAULT_TIMEOUT)
        
        # Load Perplexica docs
        with open("docs/perplexica/SEARCH.md", "r") as f:
            self.perplexica_docs = f.read()
            
        # Internal state
        self.found_sources: Dict[str, WebSource] = {}
        self.seen_urls: Set[str] = set()
        self.search_stats: Dict[str, Any] = {
            "queries_executed": 0,
            "sources_found": 0,
            "sources_filtered": 0
        }
        
    def _plan_search_strategy(self) -> List[SearchIntent]:
        """
        Plan multiple search intents based on query and context.
        
        Returns:
            List of SearchIntent objects
        """
        query = self.context.get("refined_query", "")
        user_background = self.context.get("user_background", {})
        
        intents = []
        
        # Core technical search
        intents.append(SearchIntent(
            query=query,
            focus="technical",
            required_terms=self._extract_key_terms(query),
            excluded_terms=[],
            time_range="1y"  # Default to last year
        ))
        
        # Academic focus for specialized users
        if user_background.get("user_type") == "Specialized Professional":
            academic_query = f"{query} research paper"
            intents.append(SearchIntent(
                query=academic_query,
                focus="academic",
                required_terms=["paper", "research", "study"],
                excluded_terms=[],
                domain_filters=[".edu", ".org", "arxiv.org", "scholar.google.com"]
            ))
            
        # Recent developments
        recent_query = f"{query} latest developments"
        intents.append(SearchIntent(
            query=recent_query,
            focus="news",
            time_range="3m",  # Last 3 months
            required_terms=["new", "latest", "recent"],
            excluded_terms=[]
        ))
        
        return intents

    def _extract_key_terms(self, query: str) -> List[str]:
        """Extract key terms from query for required terms"""
        # TODO: Implement more sophisticated term extraction
        # For now, just split and filter common words
        common_words = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to"}
        return [word for word in query.lower().split() 
                if word not in common_words and len(word) > 2]

    async def execute_perplexica_search(
        self,
        intent: SearchIntent
    ) -> List[Dict[str, Any]]:
        """
        Execute a search using Perplexica's API.
        
        Args:
            intent: Search intent to execute
            
        Returns:
            List of raw search results
        """
        try:
            # Configure Perplexica
            config = PerplexicaConfig(
                chat_model={
                    "provider": "openai",
                    "name": "gpt-4o-mini"  # Using mini for speed
                },
                embedding_model={
                    "provider": "openai",
                    "name": "text-embedding-3-large"
                },
                optimization_mode="balanced",
                focus_mode=self._map_intent_to_focus(intent.focus),
                stream=False
            )
            
            # Build search payload
            payload = {
                "query": intent.query,
                "chatModel": config.chat_model,
                "embeddingModel": config.embedding_model,
                "optimizationMode": config.optimization_mode,
                "focusMode": config.focus_mode
            }
            
            # Add time range if specified
            if intent.time_range:
                payload["timeRange"] = intent.time_range
                
            # Execute search
            async with self.client.post(
                "http://localhost:3000/api/search",
                json=payload,
                headers={"Authorization": f"Bearer {self.api_key}"}
            ) as response:
                if response.status_code == 200:
                    data = response.json()
                    self.search_stats["queries_executed"] += 1
                    return data.get("sources", [])
                else:
                    print(f"Perplexica search failed: {response.status_code}")
                    return []
                    
        except Exception as e:
            print(f"Error in Perplexica search: {e}")
            traceback.print_exc()
            return []

    def _map_intent_to_focus(self, intent_focus: str) -> str:
        """Map search intent focus to Perplexica focus mode"""
        focus_map = {
            "technical": "webSearch",
            "academic": "academicSearch",
            "news": "webSearch"  # Default to web search for news
        }
        return focus_map.get(intent_focus, "webSearch")

    async def process_source(
        self,
        raw_source: Dict[str, Any],
        intent: SearchIntent,
        depth: int = 0
    ) -> Optional[WebSource]:
        """
        Process and validate a single source.
        
        Args:
            raw_source: Raw source data from Perplexica
            intent: Original search intent
            depth: Discovery depth (0 = direct result)
            
        Returns:
            WebSource object if valid, None if filtered out
        """
        try:
            url = raw_source.get("metadata", {}).get("url")
            if not url or url in self.seen_urls:
                return None
                
            self.seen_urls.add(url)
            
            # Extract metadata
            domain = urlparse(url).netloc
            content_type = self._detect_content_type(raw_source)
            
            # Compute quality signals
            quality_signals = self._compute_quality_signals(raw_source, domain)
            credibility_score = self._calculate_credibility(quality_signals)
            
            if credibility_score < MIN_QUALITY_SCORE:
                return None
                
            # Create source object
            source = WebSource(
                url=url,
                title=raw_source.get("metadata", {}).get("title", ""),
                author=raw_source.get("metadata", {}).get("author"),
                date_published=raw_source.get("metadata", {}).get("date"),
                date_retrieved=datetime.now().isoformat(),
                content_type=content_type,
                domain=domain,
                content_snippet=raw_source.get("pageContent", "")[:500],
                keywords=self._extract_keywords(raw_source),
                quality_signals=quality_signals,
                credibility_score=credibility_score,
                relevance_score=self._calculate_relevance(
                    raw_source,
                    intent.query,
                    intent.required_terms
                ),
                depth=depth
            )
            
            self.search_stats["sources_found"] += 1
            return source
            
        except Exception as e:
            print(f"Error processing source {url}: {e}")
            return None

    def _detect_content_type(self, source: Dict[str, Any]) -> str:
        """Detect the type of content (article, paper, etc)"""
        url = source.get("metadata", {}).get("url", "").lower()
        content = source.get("pageContent", "").lower()
        
        if any(d in url for d in [".edu", "arxiv.org", "scholar.google"]):
            return "academic"
        elif any(term in content for term in ["paper", "research", "study", "journal"]):
            return "paper"
        elif "github.com" in url:
            return "code"
        elif any(d in url for d in ["blog.", ".blog", "medium.com"]):
            return "blog"
        else:
            return "article"

    def _compute_quality_signals(
        self,
        source: Dict[str, Any],
        domain: str
    ) -> Dict[str, Any]:
        """Compute various quality signals for a source"""
        signals = {
            "domain_authority": self._get_domain_authority(domain),
            "content_length": len(source.get("pageContent", "")),
            "has_references": "references" in source.get("pageContent", "").lower(),
            "reading_level": self._estimate_reading_level(source.get("pageContent", "")),
            "technical_depth": self._estimate_technical_depth(source)
        }
        return signals

    def _get_domain_authority(self, domain: str) -> float:
        """Estimate domain authority (placeholder)"""
        # TODO: Implement real domain authority checking
        edu_bonus = 0.2 if ".edu" in domain else 0
        org_bonus = 0.1 if ".org" in domain else 0
        return 0.7 + edu_bonus + org_bonus

    def _estimate_reading_level(self, content: str) -> float:
        """Estimate content reading level (placeholder)"""
        # TODO: Implement sophisticated reading level analysis
        return 0.8  # Default to moderately complex

    def _estimate_technical_depth(self, source: Dict[str, Any]) -> float:
        """Estimate technical depth of content (placeholder)"""
        # TODO: Implement real technical depth analysis
        return 0.7  # Default to moderately technical

    def _calculate_credibility(self, signals: Dict[str, Any]) -> float:
        """Calculate overall credibility score from signals"""
        weights = {
            "domain_authority": 0.4,
            "content_length": 0.1,
            "has_references": 0.2,
            "reading_level": 0.15,
            "technical_depth": 0.15
        }
        
        score = sum(
            signals.get(signal, 0) * weight
            for signal, weight in weights.items()
        )
        
        return min(1.0, max(0.0, score))

    def _calculate_relevance(
        self,
        source: Dict[str, Any],
        query: str,
        required_terms: List[str]
    ) -> float:
        """Calculate relevance score for a source"""
        content = source.get("pageContent", "").lower()
        title = source.get("metadata", {}).get("title", "").lower()
        
        # Check required terms
        if not all(term.lower() in content or term.lower() in title 
                  for term in required_terms):
            return 0.0
            
        # Simple term frequency for now
        query_terms = set(query.lower().split())
        matches_title = sum(1 for term in query_terms if term in title)
        matches_content = sum(1 for term in query_terms if term in content)
        
        # Weight title matches more heavily
        score = (matches_title * 2 + matches_content) / (len(query_terms) * 3)
        return min(1.0, score)

    def _extract_keywords(self, source: Dict[str, Any]) -> List[str]:
        """Extract keywords from source content"""
        # TODO: Implement more sophisticated keyword extraction
        # For now return any provided keywords or empty list
        return source.get("metadata", {}).get("keywords", [])

    async def discover_related_content(
        self,
        source: WebSource,
        intent: SearchIntent,
        depth: int
    ) -> List[WebSource]:
        """
        Discover content related to a source through links.
        
        Args:
            source: Original source
            intent: Search intent
            depth: Current depth
            
        Returns:
            List of related sources
        """
        if depth >= MAX_DEPTH:
            return []
            
        related = []
        try:
            # Get page content
            async with self.client.get(source.url) as response:
                if response.status_code != 200:
                    return []
                    
                content = response.text
                
            # Extract links
            links = self._extract_links(content, source.url)
            
            # Process each link
            for link in links[:5]:  # Limit to top 5 links
                if link not in self.seen_urls:
                    # Fetch and process linked page
                    async with self.client.get(link) as response:
                        if response.status_code == 200:
                            raw_source = {
                                "metadata": {
                                    "url": link,
                                    "title": self._extract_title(response.text)
                                },
                                "pageContent": response.text
                            }
                            
                            related_source = await self.process_source(
                                raw_source,
                                intent,
                                depth + 1
                            )
                            
                            if related_source:
                                related.append(related_source)
                                
        except Exception as e:
            print(f"Error discovering related content: {e}")
            
        return related

    def _extract_links(self, content: str, base_url: str) -> List[str]:
        """Extract and normalize links from HTML content"""
        # TODO: Implement proper HTML parsing
        # For now return empty list
        return []

    def _extract_title(self, content: str) -> str:
        """Extract title from HTML content"""
        # TODO: Implement proper HTML parsing
        # For now return empty string
        return ""

    async def execute_search_strategy(self) -> Dict[str, Any]:
        """
        Execute the full web search strategy.
        
        Steps:
        1. Plan search intents
        2. Execute Perplexica searches
        3. Process and validate sources
        4. Discover related content
        5. Rank results
        
        Returns:
            Dict containing found sources and statistics
        """
        stats = {"started": datetime.now().isoformat()}
        
        try:
            # Plan search strategy
            intents = self._plan_search_strategy()
            stats["intents_planned"] = len(intents)
            
            # Execute all intents
            for intent in intents:
                # Search with Perplexica
                results = await self.execute_perplexica_search(intent)
                
                # Process sources
                for result in results:
                    source = await self.process_source(result, intent)
                    if source:
                        self.found_sources[source.url] = source
                        
                        # Discover related content if depth allows
                        if len(self.found_sources) < 50:  # Limit total sources
                            related = await self.discover_related_content(
                                source,
                                intent,
                                depth=0
                            )
                            
                            # Add related sources
                            for rel in related:
                                if rel.url not in self.found_sources:
                                    self.found_sources[rel.url] = rel
                                    
            stats.update(self.search_stats)
            stats["completed"] = datetime.now().isoformat()
            
            return {
                "sources": self.found_sources,
                "statistics": stats
            }
            
        except Exception as e:
            print(f"Error in search strategy: {e}")
            traceback.print_exc()
            stats["error"] = str(e)
            stats["completed"] = datetime.now().isoformat()
            return {
                "sources": {},
                "statistics": stats
            }

--------------------------------------------------
File End
--------------------------------------------------


.\processes\sourcing\agent.py
File type: .py
"""
Source Agent implementation for Vizier.

This module implements an autonomous agent capable of deep source analysis,
insight extraction, and maintaining context for writer clarifications.
"""

import json 
import asyncio
from typing import Dict, List, Optional, Union, Any, Literal, Set
from dataclasses import dataclass
from bs4 import BeautifulSoup
import httpx
import PyPDF2
import io
import trafilatura
import tiktoken
from urllib.parse import urlparse
from pydantic import BaseModel, Field
from datetime import datetime
from routers.openrouter import OpenRouterClient

DEFAULT_MODEL = "openrouter/anthropic/claude-3-opus"
TOKENS_PER_CHUNK = 2048
MAX_CHUNKS = 20  # Maximum number of chunks we'll process at once
SUMMARY_MAX_TOKENS = 4000
MAX_TOKENS = 100000
EXPLORATION_TEMPERATURE = 0.3
CLARIFICATION_TEMPERATURE = 0.7

class ProcessedQuote(BaseModel):
    """A quote extracted from source content"""
    content: str = Field(description="The quoted text")
    context: str = Field(description="Context around the quote")
    relevance: float = Field(description="Relevance score (0-1)")
    themes: List[str] = Field(description="Themes this quote relates to")

class SourceInsight(BaseModel):
    """An insight extracted from source analysis"""
    content: str = Field(description="The insight content")
    confidence: float = Field(description="Confidence score (0-1)")
    related_insights: List[str] = Field(description="Related insights to explore")
    supporting_quotes: List[str] = Field(description="Supporting quotes")
    themes: List[str] = Field(description="Themes this insight relates to")

class ProcessedSource(BaseModel):
    """A fully processed source with extracted insights"""
    source_id: str = Field(description="Unique identifier for the source")
    source_type: str = Field(description="Type of source (web, academic, etc)")
    confidence_score: float = Field(description="Overall confidence in analysis (0-1)")
    complexity_score: float = Field(description="Technical complexity score (0-1)")
    domain_tags: List[str] = Field(description="Domain-specific tags")
    timestamp: str = Field(description="Processing timestamp")
    potential_clarifications: List[str] = Field(description="Potential areas for clarification")
    key_insights: List[SourceInsight] = Field(description="Key insights extracted")
    major_themes: List[str] = Field(description="Major themes identified")
    quoted_content: Dict[str, ProcessedQuote] = Field(description="Extracted quotes")
    summary: str = Field(description="Brief summary of the source")

class ExplorationPlan(BaseModel):
    """Plan for exploring a source's content"""
    content_type: str = Field(description="Type of content being analyzed")
    key_areas: List[str] = Field(description="Key areas to explore")
    exploration_steps: List[Dict[str, Any]] = Field(description="Planned exploration steps")
    priority_themes: List[str] = Field(description="Themes to prioritize")

class SourceAgent:
    """An autonomous agent for deep source analysis and insight extraction"""
    
    def __init__(
        self,
        meta_prompt: str,
        source_urls: List[str],
        role_context: str,
        objectives: List[str],
        model: str = DEFAULT_MODEL
    ):
        """
        Initialize a source agent.
        
        Args:
            meta_prompt: High-level guidance for the agent
            source_urls: List of source URLs to process
            role_context: Context about the agent's role
            objectives: List of analysis objectives
            model: LLM model to use
        """
        self.client = OpenRouterClient()
        self.model = model
        self.meta_prompt = meta_prompt
        self.source_urls = source_urls
        self.role_context = role_context
        self.objectives = objectives
        
        # Internal state
        self.processed_sources: Dict[str, ProcessedSource] = {}
        self.exploration_history: List[Dict[str, Any]] = []
        self.clarification_cache: Dict[str, Dict[str, Any]] = {}
        self.theme_expertise: Set[str] = set()
        self.confidence_thresholds: Dict[str, float] = {
            "insight": 0.7,
            "quote": 0.8,
            "clarification": 0.75
        }

    async def _plan_exploration(self, content: str) -> ExplorationPlan:
        """Create a plan for exploring source content"""
        prompt = f"""
        Create an exploration plan for analyzing this content.

        Content Type: Auto-detect from content
        Content Preview: {content[:1000]}...

        Role Context: {self.role_context}
        Objectives: {json.dumps(self.objectives)}

        Return a JSON exploration plan including:
        1. Content type classification
        2. Key areas to explore
        3. Specific exploration steps
        4. Priority themes to focus on
        """

        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=EXPLORATION_TEMPERATURE,
                response_format={"type": "json_object"}
            )
            
            plan_dict = json.loads(response["choices"][0]["message"]["content"])
            return ExplorationPlan(**plan_dict)
            
        except Exception as e:
            print(f"Error creating exploration plan: {e}")
            return ExplorationPlan(
                content_type="unknown",
                key_areas=[],
                exploration_steps=[],
                priority_themes=[]
            )

    async def _extract_quotes(self, content: str, themes: List[str]) -> Dict[str, ProcessedQuote]:
        """Extract relevant quotes from content"""
        prompt = f"""
        Extract key quotes from the content that support our analysis objectives.
        For each quote:
        1. Include surrounding context
        2. Rate relevance (0-1)
        3. Tag with relevant themes
        4. Only include quotes with relevance > {self.confidence_thresholds['quote']}

        Content: {content}
        Themes to Consider: {themes}
        """

        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=MAX_TOKENS,
                temperature=EXPLORATION_TEMPERATURE,
                response_format={"type": "json_object"}
            )
            
            quotes_dict = json.loads(response["choices"][0]["message"]["content"])
            return {
                qid: ProcessedQuote(**quote_data)
                for qid, quote_data in quotes_dict.items()
            }
            
        except Exception as e:
            print(f"Error extracting quotes: {e}")
            return {}

    async def _identify_insights(
        self,
        content: str,
        quotes: Dict[str, ProcessedQuote]
    ) -> List[SourceInsight]:
        """Identify key insights from content and quotes"""
        prompt = f"""
        Based on the content and extracted quotes, identify key insights that:
        1. Address our analysis objectives
        2. Have confidence > {self.confidence_thresholds['insight']}
        3. Are supported by specific quotes
        4. Connect to broader themes
        5. Suggest related areas to explore

        Content: {content}
        Extracted Quotes: {json.dumps(quotes)}
        Objectives: {json.dumps(self.objectives)}
        """

        try:
            response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=MAX_TOKENS,
                temperature=EXPLORATION_TEMPERATURE,
                response_format={"type": "json_object"}
            )
            
            insights_list = json.loads(response["choices"][0]["message"]["content"])
            return [SourceInsight(**insight_data) for insight_data in insights_list]
            
        except Exception as e:
            print(f"Error identifying insights: {e}")
            return []

    async def process_source(self, source_id: str, url: str) -> ProcessedSource:
        """Process a single source through the full pipeline with deep exploration"""
        try:
            # Extract content (placeholder - implement actual extraction)
            content = await self.extract_source_content(url)
            if not content:
                raise ValueError(f"Could not extract content from {url}")
                
            # Create exploration plan
            plan = await self._plan_exploration(content)
            
            # Extract quotes
            quotes = await self._extract_quotes(content, plan.priority_themes)
            
            # Identify insights
            insights = await self._identify_insights(content, quotes)
            
            # Calculate aggregate metrics
            avg_confidence = sum(i.confidence for i in insights) / len(insights) if insights else 0
            
            # Update theme expertise
            self.theme_expertise.update(plan.priority_themes)
            
            # Generate summary based on insights
            summary_prompt = f"""
            Create a brief summary of this source based on the extracted insights.
            Focus on key findings relevant to our objectives.

            Insights: {json.dumps([i.dict() for i in insights])}
            """
            
            summary_response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": summary_prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            summary = summary_response["choices"][0]["message"]["content"]
            
            # Create processed source
            processed = ProcessedSource(
                source_id=source_id,
                source_type=plan.content_type,
                confidence_score=avg_confidence,
                complexity_score=0.7,  # TODO: Implement complexity scoring
                domain_tags=plan.key_areas,
                timestamp=datetime.now().isoformat(),
                potential_clarifications=[c for i in insights for c in i.related_insights],
                key_insights=insights,
                major_themes=plan.priority_themes,
                quoted_content=quotes,
                summary=summary
            )
            
            self.processed_sources[source_id] = processed
            return processed
            
        except Exception as e:
            print(f"Error processing source {url}: {e}")
            raise

    async def process_all_sources(self) -> Dict[str, ProcessedSource]:
        """Process all assigned sources"""
        results = {}
        for i, url in enumerate(self.source_urls):
            source_id = f"source_{i}"
            try:
                processed = await self.process_source(source_id, url)
                results[source_id] = processed
            except Exception as e:
                print(f"Error processing source {url}: {e}")
                continue
        return results

    async def get_clarification(self, query: str, source_id: str) -> str:
        """
        Respond to a clarification request about a specific source with 
        thorough exploration and verification of the response.
        """
        if source_id not in self.processed_sources:
            return f"Source {source_id} not found"
            
        # Check cache first
        cache_key = f"{source_id}:{query}"
        if cache_key in self.clarification_cache:
            return self.clarification_cache[cache_key]["response"]
            
        source = self.processed_sources[source_id]
        
        # Find relevant insights and quotes
        relevant_insights = [
            i for i in source.key_insights
            if any(theme in query.lower() for theme in i.themes)
        ]
        
        relevant_quotes = [
            quote for quote in source.quoted_content.values()
            if any(theme in query.lower() for theme in quote.themes)
        ]
        
        clarification_prompt = f"""
        Answer this clarification request about source {source_id}:
        
        Query: {query}
        
        Source Summary: {source.summary}
        
        Relevant Insights: 
        {json.dumps([i.dict() for i in relevant_insights], indent=2)}
        
        Supporting Quotes:
        {json.dumps([q.dict() for q in relevant_quotes], indent=2)}
        
        Requirements:
        1. Answer specifically based on source content
        2. Support claims with quotes where possible
        3. Note if information is uncertain or needs additional context
        4. Only include high-confidence information
        5. Suggest related areas to explore if relevant
        """
        
        try:
            # Generate initial response
            response = await self.client.chat_completion(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.meta_prompt},
                    {"role": "user", "content": clarification_prompt}
                ],
                max_tokens=1000,
                temperature=CLARIFICATION_TEMPERATURE
            )
            
            clarification = response["choices"][0]["message"]["content"]
            
            # Verify response quality
            verification_prompt = f"""
            Verify this clarification response:
            
            Original Query: {query}
            Response: {clarification}
            
            Check:
            1. Does it directly answer the query?
            2. Is it supported by source content?
            3. Is confidence properly qualified?
            4. Are important caveats noted?
            
            Return a confidence score (0-1) and any issues found.
            """
            
            verify_response = await self.client.chat_completion(
                model=self.model,
                messages=[{"role": "user", "content": verification_prompt}],
                max_tokens=500,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            verification = json.loads(verify_response["choices"][0]["message"]["content"])
            confidence = verification.get("confidence", 0)
            
            # Only cache if confidence meets threshold
            if confidence >= self.confidence_thresholds["clarification"]:
                self.clarification_cache[cache_key] = {
                    "response": clarification,
                    "confidence": confidence,
                    "timestamp": datetime.now().isoformat()
                }
            
            return clarification
            
        except Exception as e:
            print(f"Error generating clarification: {e}")
            return f"Error clarifying source {source_id}: {str(e)}"

    async def extract_source_content(self, url: str) -> Optional[str]:
        """Extract content from a source URL"""
        # TODO: Implement actual content extraction
        # This is a placeholder that should be replaced with real extraction logic
        return f"Sample content from {url}"

--------------------------------------------------
File End
--------------------------------------------------


.\processes\sourcing\director.py
File type: .py
"""
Source Director Module

Coordinates multiple source agents and manages their interactions with the writer.
"""

import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
import json
from pydantic import BaseModel, Field

from .agent import SourceAgent, ProcessedSource
from ..connectors.router_04 import WritingContextResponse

class DirectorRequest(BaseModel):
    """Request to the director for source agent coordination"""
    agent_id: str = Field(description="ID of the source agent to query")
    source_id: str = Field(description="ID of the source to get clarification about")
    query: str = Field(description="The clarification query")
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context for the query")

class DirectorResponse(BaseModel):
    """Response from the director containing source agent results"""
    agent_id: str = Field(description="ID of the responding source agent")
    source_id: str = Field(description="ID of the source that was clarified")
    clarification: str = Field(description="The clarification response")
    confidence: float = Field(description="Confidence score for the response (0-1)")
    supporting_quotes: List[str] = Field(description="Supporting quotes from the source")

class AgentState(BaseModel):
    """Track the state and performance of a source agent"""
    agent_id: str = Field(description="Unique identifier for the agent")
    assigned_sources: List[str] = Field(description="Sources assigned to this agent")
    active_queries: int = Field(default=0, description="Number of active queries")
    completed_queries: int = Field(default=0, description="Number of completed queries")
    avg_response_time: float = Field(default=0.0, description="Average response time in seconds")
    last_active: datetime = Field(default_factory=datetime.now, description="Last activity timestamp")

class SourceDirector:
    """
    Coordinates source agents and manages their interactions with the writer.
    
    Key responsibilities:
    1. Track source agent states and capabilities
    2. Route queries to appropriate agents
    3. Handle parallel processing of multiple queries
    4. Manage agent load balancing
    5. Monitor agent performance
    6. Handle failover if needed
    """
    
    def __init__(self):
        self.agents: Dict[str, SourceAgent] = {}
        self.agent_states: Dict[str, AgentState] = {}
        self.source_assignments: Dict[str, str] = {}  # source_id -> agent_id
        self.query_history: List[Dict[str, Any]] = []
        self.max_parallel_queries = 5
        self.query_semaphore = asyncio.Semaphore(self.max_parallel_queries)

    async def register_agent(
        self, 
        agent_id: str, 
        agent: SourceAgent, 
        assigned_sources: List[str]
    ) -> None:
        """Register a new source agent with the director"""
        self.agents[agent_id] = agent
        self.agent_states[agent_id] = AgentState(
            agent_id=agent_id,
            assigned_sources=assigned_sources
        )
        
        # Update source assignments
        for source_id in assigned_sources:
            self.source_assignments[source_id] = agent_id

    def _get_agent_for_source(self, source_id: str) -> Optional[str]:
        """Get the agent ID responsible for a source"""
        return self.source_assignments.get(source_id)

    def _update_agent_metrics(
        self, 
        agent_id: str, 
        response_time: float, 
        success: bool
    ) -> None:
        """Update performance metrics for an agent"""
        if agent_id in self.agent_states:
            state = self.agent_states[agent_id]
            state.completed_queries += 1
            
            # Update average response time with exponential moving average
            alpha = 0.1  # Smoothing factor
            state.avg_response_time = (
                (1 - alpha) * state.avg_response_time + 
                alpha * response_time
            )
            
            state.last_active = datetime.now()

    async def _process_query(
        self, 
        agent: SourceAgent,
        agent_id: str,
        source_id: str,
        query: str,
        context: Optional[Dict[str, Any]] = None
    ) -> DirectorResponse:
        """Process a single query through a source agent"""
        start_time = datetime.now()
        
        try:
            # Get clarification from the agent
            clarification = await agent.get_clarification(query, source_id)
            
            # Get supporting quotes if available
            source = agent.processed_sources.get(source_id)
            quotes = list(source.quoted_content.values()) if source else []
            
            # Calculate confidence based on agent's source analysis
            confidence = source.confidence_score if source else 0.5
            
            response = DirectorResponse(
                agent_id=agent_id,
                source_id=source_id,
                clarification=clarification,
                confidence=confidence,
                supporting_quotes=quotes[:3]  # Limit to top 3 quotes
            )
            
        except Exception as e:
            print(f"Error processing query through agent {agent_id}: {e}")
            response = DirectorResponse(
                agent_id=agent_id,
                source_id=source_id,
                clarification=f"Error: {str(e)}",
                confidence=0.0,
                supporting_quotes=[]
            )
            
        # Update metrics
        response_time = (datetime.now() - start_time).total_seconds()
        self._update_agent_metrics(
            agent_id,
            response_time,
            response.confidence > 0
        )
        
        return response

    async def get_clarification(self, request: DirectorRequest) -> Optional[DirectorResponse]:
        """
        Get clarification about a source from the responsible agent.
        
        Args:
            request: The clarification request
            
        Returns:
            DirectorResponse with the clarification if successful
        """
        # Validate request
        agent_id = request.agent_id
        if agent_id not in self.agents:
            print(f"Unknown agent ID: {agent_id}")
            return None
            
        source_id = request.source_id
        assigned_agent = self._get_agent_for_source(source_id)
        if assigned_agent != agent_id:
            print(f"Source {source_id} not assigned to agent {agent_id}")
            return None
            
        # Get the agent
        agent = self.agents[agent_id]
        
        # Process with concurrency control
        async with self.query_semaphore:
            try:
                # Update active queries count
                self.agent_states[agent_id].active_queries += 1
                
                # Process the query
                response = await self._process_query(
                    agent,
                    agent_id,
                    source_id,
                    request.query,
                    request.context
                )
                
                # Log the query
                self.query_history.append({
                    "timestamp": datetime.now().isoformat(),
                    "agent_id": agent_id,
                    "source_id": source_id,
                    "query": request.query,
                    "context": request.context,
                    "success": response.confidence > 0
                })
                
                return response
                
            finally:
                # Update active queries count
                self.agent_states[agent_id].active_queries -= 1

    async def process_parallel_queries(
        self, 
        requests: List[DirectorRequest]
    ) -> List[Optional[DirectorResponse]]:
        """
        Process multiple queries in parallel while respecting concurrency limits.
        
        Args:
            requests: List of clarification requests
            
        Returns:
            List of responses (None for failed requests)
        """
        async def process_single(request: DirectorRequest) -> Optional[DirectorResponse]:
            return await self.get_clarification(request)
            
        # Process all requests in parallel
        responses = await asyncio.gather(
            *[process_single(req) for req in requests],
            return_exceptions=True
        )
        
        # Filter out exceptions
        return [
            r if not isinstance(r, Exception) else None
            for r in responses
        ]

    def get_agent_status(self, agent_id: str) -> Optional[AgentState]:
        """Get the current state of a source agent"""
        return self.agent_states.get(agent_id)

    def get_agent_performance(self, agent_id: str) -> Dict[str, Any]:
        """Get performance metrics for a source agent"""
        state = self.agent_states.get(agent_id)
        if not state:
            return {}
            
        return {
            "completed_queries": state.completed_queries,
            "avg_response_time": state.avg_response_time,
            "current_load": state.active_queries
        }

    async def check_agent_health(self) -> Dict[str, bool]:
        """Check the health status of all agents"""
        results = {}
        for agent_id, state in self.agent_states.items():
            # Consider an agent unhealthy if:
            # 1. It has been inactive for too long
            # 2. Its average response time is too high
            # 3. It has too many active queries
            
            inactive_time = (datetime.now() - state.last_active).total_seconds()
            is_healthy = (
                inactive_time < 3600 and  # Less than 1 hour inactive
                state.avg_response_time < 30 and  # Average response under 30s
                state.active_queries < self.max_parallel_queries  # Not overloaded
            )
            results[agent_id] = is_healthy
            
        return results

--------------------------------------------------
File End
--------------------------------------------------


.\processes\writer\generator.py
File type: .py
"""
Draft Generator Module

This module handles the generation of draft content using approved sources.
It includes capabilities for initial draft creation and handling revisions
based on user feedback.
"""

import asyncio
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from routers.openrouter import OpenRouterClient

# Using Claude 3 for high-quality writing
WRITER_MODEL = "openrouter/anthropic/claude-3-opus"

class Source(BaseModel):
    """Represents an approved source for writing"""
    id: str
    title: str
    content: str
    url: str
    metadata: Dict[str, Any]

class WritingRequest(BaseModel):
    """Input for draft generation"""
    query: str
    sources: List[Source]
    style_guide: Optional[Dict[str, Any]] = None
    previous_feedback: Optional[str] = None

async def generate_draft(request: WritingRequest) -> str:
    """Generate a draft based on approved sources"""
    client = OpenRouterClient()
    try:
        # Create the system prompt
        system_prompt = """You are an expert research writer tasked with creating a comprehensive 
        report based on provided sources. Follow these guidelines:
        1. Use factual information from sources
        2. Cite sources appropriately
        3. Maintain a clear, professional tone
        4. Organize content logically
        5. Highlight key findings and insights
        """
        
        if request.style_guide:
            system_prompt += f"\nAdditional style requirements:\n{request.style_guide}"
        
        # Format sources for the prompt
        sources_text = "\n\n".join([
            f"Source {i+1}: {s.title}\nURL: {s.url}\nContent: {s.content[:1000]}..."
            for i, s in enumerate(request.sources)
        ])
        
        user_prompt = f"""Query: {request.query}

Available Sources:
{sources_text}

{f'Previous feedback to address: {request.previous_feedback}' if request.previous_feedback else ''}

Write a comprehensive research report addressing the query using these sources."""

        # Generate the draft
        response = await client.chat_completion(
            model=WRITER_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=4000,
            temperature=0.7
        )
        
        return response.choices[0].message.content
        
    finally:
        await client.client.aclose()

async def revise_draft(
    original_draft: str,
    feedback: str,
    sources: List[Source]
) -> str:
    """Revise a draft based on user feedback"""
    client = OpenRouterClient()
    try:
        system_prompt = """You are an expert editor tasked with revising a draft based on feedback.
        Maintain the draft's core content while addressing all feedback points thoroughly."""
        
        user_prompt = f"""Original Draft:
{original_draft}

User Feedback:
{feedback}

Available Sources:
""" + "\n\n".join([f"Source {i+1}: {s.title}\nURL: {s.url}" for i, s in enumerate(sources)])

        response = await client.chat_completion(
            model=WRITER_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=4000,
            temperature=0.7
        )
        
        return response.choices[0].message.content
        
    finally:
        await client.client.aclose()

--------------------------------------------------
File End
--------------------------------------------------


.\routers\auth.py
File type: .py
from fastapi import APIRouter, Request, Depends, HTTPException
from fastapi.responses import RedirectResponse
from jose import jwt
from httpx import AsyncClient
import os
import uuid
from uuid import UUID
from dotenv import load_dotenv
from database import database

load_dotenv()

router = APIRouter(prefix="/auth", tags=["auth"])

GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
REDIRECT_URI = os.getenv("REDIRECT_URI")
JWT_SECRET = os.getenv("JWT_SECRET")




'''url for frontend/testing to use: https://accounts.google.com/o/oauth2/v2/auth?
response_type=code&
client_id=YOUR_GOOGLE_CLIENT_ID&
redirect_uri=http://localhost:8000/auth/callback&
scope=email%20profile&
access_type=offline&
prompt=consent'''

@router.get("/callback")
async def google_auth_callback(code: str):
    print("ðŸ” Step 1: Received code from Google:", code)

    # exchange code for tokens
    async with AsyncClient() as client:
        print("ðŸŒ Step 2: Exchanging code for access token...")
        token_res = await client.post("https://oauth2.googleapis.com/token", data={
            "code": code,
            "client_id": GOOGLE_CLIENT_ID,
            "client_secret": GOOGLE_CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI,
            "grant_type": "authorization_code"
        })
        token_data = token_res.json()
        access_token = token_data.get("access_token")

        print("ðŸ”‘ Access token received:", access_token)

        if not access_token:
            print("âŒ Failed to get access token:", token_data)
            raise HTTPException(status_code=401, detail="Invalid code/token exchange")

        # get user info
        print("ðŸ‘¤ Step 3: Fetching user info from Google...")
        userinfo_res = await client.get("https://www.googleapis.com/oauth2/v2/userinfo",
                                        headers={"Authorization": f"Bearer {access_token}"})
        userinfo = userinfo_res.json()
        print("âœ… User info received:", userinfo)

        email = userinfo["email"]
        google_id = userinfo["id"]
        name = userinfo.get("name", "")
        picture = userinfo.get("picture", "")

    # Step 4: Check DB for user
    query = "SELECT * FROM users WHERE email = :email"
    user = await database.fetch_one(query, {"email": email})

    if not user:
        print("âž• Creating new user in DB")
        user_id = str(uuid.uuid4())
        insert_query = """
        INSERT INTO users (
            user_id, email, name, google_id, picture_url,
            archetype, user_description, user_goals, user_experience, acct_created_date
        ) VALUES (
            :user_id, :email, :name, :google_id, :picture,
            NULL, NULL, NULL, NULL, NOW()
        )
        """
        await database.execute(insert_query, {
            "user_id": user_id,
            "email": email,
            "name": name,
            "google_id": google_id,
            "picture": picture
        })
    else:
        print("âœ… User found in DB")
        user_id = user["user_id"]

    print("ðŸ” Step 5: Issuing JWT for user:", user_id)
    token = jwt.encode({"sub": str(user_id)}, JWT_SECRET, algorithm="HS256")

    print("ðŸŽ‰ Step 6: Returning JWT to client")
    #return {"access_token": token}
    print(token)
    return RedirectResponse(f"http://localhost:5173/login/success?token={token}")
    #change this to redirect response    


@router.get("/me")
async def get_me(request: Request):
    auth_header = request.headers.get("authorization")
    print("ðŸ” Raw Authorization header:", auth_header)

    try:
        token = auth_header.split(" ")[1]
        payload = jwt.decode(token, JWT_SECRET, algorithms=["HS256"])
        return {"user_id": UUID(payload["sub"])}
    except Exception as e:
        print("âŒ Invalid token:", e)
        raise HTTPException(status_code=401, detail="Invalid token")


--------------------------------------------------
File End
--------------------------------------------------


.\routers\drafts.py
File type: .py
from fastapi import APIRouter, Request, HTTPException, Depends
from fastapi.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
from database import database
from routers.queries import get_current_user, ProcessStage, active_sessions
from routers.openrouter import OpenRouterClient, get_openrouter_client
import asyncio
import json
from typing import Optional, Dict, Any
from datetime import datetime
import uuid

router = APIRouter(prefix="/drafts", tags=["drafts"])

class DraftState:
    def __init__(self, draft_id: str):
        self.draft_id = draft_id
        self.content = ""
        self.is_completed = False
        self._event_queue = asyncio.Queue()
        
    async def update_content(self, new_content: str, is_final: bool = False):
        event = {
            "type": "content_update",
            "content": new_content,
            "timestamp": str(datetime.utcnow())
        }
        await self._event_queue.put(event)
        self.content = new_content
        
        if is_final:
            self.is_completed = True
            
    async def event_generator(self):
        while not self.is_completed or not self._event_queue.empty():
            try:
                event = await self._event_queue.get()
                yield event
            except Exception as e:
                print(f"Error in draft event generator: {e}")
                break

# Global draft states
active_drafts: Dict[str, DraftState] = {}

@router.post("/generate")
async def generate_draft(
    data: dict,
    request: Request, 
    openrouter: OpenRouterClient = Depends(get_openrouter_client)
):
    """Generate a new draft from query results"""
    user_id = await get_current_user(request)
    query_id = data.get("query_id")
    
    if not query_id:
        raise HTTPException(status_code=400, detail="query_id is required")
        
    try:
        # Get query and sources
        query = await database.fetch_one("""
            SELECT refined_query, sources FROM queries
            WHERE query_id = :query_id AND user_id = :user_id
        """, {
            "query_id": query_id,
            "user_id": user_id
        })
        
        if not query:
            raise HTTPException(status_code=404, detail="Query not found")
        
        draft_id = str(uuid.uuid4())
        
        # Initialize draft state
        draft_state = DraftState(draft_id)
        active_drafts[draft_id] = draft_state
        
        # Update query session state
        if query_id in active_sessions:
            await active_sessions[query_id].update_stage(ProcessStage.WRITING_STARTED)
        
        # Create draft record
        await database.execute("""
            INSERT INTO drafts (draft_id, query_id, user_id, status)
            VALUES (:draft_id, :query_id, :user_id, 'writing')
        """, {
            "draft_id": draft_id,
            "query_id": query_id,
            "user_id": user_id
        })
        
        # Start async draft generation
        asyncio.create_task(generate_draft_content(
            draft_state,
            query["refined_query"],
            query["sources"],
            openrouter,
            query_id
        ))
        
        return {"draft_id": draft_id}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def generate_draft_content(
    draft_state: DraftState,
    refined_query: str,
    sources: list,
    openrouter: OpenRouterClient,
    query_id: str
):
    """Generate draft content using streaming completion"""
    try:
        messages = [{
            "role": "system",
            "content": "You are a research writing assistant. Create a well-structured draft based on the refined query and sources."
        }, {
            "role": "user",
            "content": f"Write a research draft answering this query: {refined_query}\n\nSources:\n{json.dumps(sources, indent=2)}"
        }]
        
        full_content = ""
        async for chunk in await openrouter.chat_completion(
            model="anthropic/claude-3-sonnet",
            messages=messages,
            stream=True,
            temperature=0.7,
            max_tokens=4000
        ):
            if chunk and chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_content += content
                await draft_state.update_content(full_content)
                
        # Mark as complete
        await draft_state.update_content(full_content, is_final=True)
        
        # Update database
        await database.execute("""
            UPDATE drafts 
            SET content = :content, status = 'completed'
            WHERE draft_id = :draft_id
        """, {
            "content": full_content,
            "draft_id": draft_state.draft_id
        })
        
        # Update query session
        if query_id in active_sessions:
            await active_sessions[query_id].update_stage(ProcessStage.DRAFT_READY)
            
    except Exception as e:
        print(f"Error generating draft: {e}")
        if query_id in active_sessions:
            await active_sessions[query_id].update_stage(
                ProcessStage.DRAFT_READY,
                {"error": str(e)}
            )

@router.get("/{draft_id}")
async def get_draft(draft_id: str, request: Request):
    """Get draft details and content"""
    user_id = await get_current_user(request)
    
    try:
        draft = await database.fetch_one("""
            SELECT * FROM drafts
            WHERE draft_id = :draft_id AND user_id = :user_id
        """, {
            "draft_id": draft_id,
            "user_id": user_id
        })
        
        if not draft:
            raise HTTPException(status_code=404, detail="Draft not found")
            
        return dict(draft)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/{draft_id}/accept")
async def accept_draft(draft_id: str, request: Request):
    """Accept and finalize a draft"""
    user_id = await get_current_user(request)
    
    try:
        # Get draft and associated query
        draft = await database.fetch_one("""
            SELECT query_id FROM drafts
            WHERE draft_id = :draft_id AND user_id = :user_id
        """, {
            "draft_id": draft_id,
            "user_id": user_id
        })
        
        if not draft:
            raise HTTPException(status_code=404, detail="Draft not found")
            
        # Update draft status    
        await database.execute("""
            UPDATE drafts SET status = 'accepted'
            WHERE draft_id = :draft_id
        """, {"draft_id": draft_id})
        
        # Update query session
        query_id = draft["query_id"]
        if query_id in active_sessions:
            await active_sessions[query_id].update_stage(ProcessStage.DRAFT_APPROVED)
            await active_sessions[query_id].update_stage(ProcessStage.COMPLETED)
            
        return {"status": "accepted"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/{draft_id}/reject")
async def reject_draft(draft_id: str, data: dict, request: Request):
    """Reject a draft with feedback"""
    user_id = await get_current_user(request)
    
    try:
        # Get draft
        draft = await database.fetch_one("""
            SELECT query_id FROM drafts
            WHERE draft_id = :draft_id AND user_id = :user_id
        """, {
            "draft_id": draft_id,
            "user_id": user_id
        })
        
        if not draft:
            raise HTTPException(status_code=404, detail="Draft not found")
            
        feedback = data.get("feedback", "")
            
        # Update draft status
        await database.execute("""
            UPDATE drafts 
            SET status = 'rejected', feedback = :feedback
            WHERE draft_id = :draft_id
        """, {
            "draft_id": draft_id,
            "feedback": feedback
        })
        
        return {"status": "rejected"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stream/{draft_id}")
async def stream_draft(draft_id: str, request: Request):
    """Stream draft content updates using SSE"""
    if draft_id not in active_drafts:
        raise HTTPException(status_code=404, detail="Draft generation not found")
        
    async def event_generator():
        draft_state = active_drafts[draft_id]
        try:
            async for event in draft_state.event_generator():
                if await request.is_disconnected():
                    break
                yield {
                    "event": "message",
                    "data": json.dumps(event)
                }
        except Exception as e:
            print(f"Error in draft stream: {e}")
        finally:
            if draft_state.is_completed:
                active_drafts.pop(draft_id, None)
    
    return EventSourceResponse(event_generator())


--------------------------------------------------
File End
--------------------------------------------------


.\routers\openrouter.py
File type: .py
"""
OpenRouter API client for interacting with AI models through OpenRouter's service.

This module provides a clean interface for:
- Listing available models with filtering options
- Making chat completion requests with full parameter support
- Using tools/function calling with models
"""

import os
from typing import Dict, List, Optional, Union, Any, Literal
import httpx
import json
from pydantic import BaseModel
from dotenv import load_dotenv, find_dotenv
from enum import Enum
from fastapi import APIRouter, Depends, HTTPException

load_dotenv(find_dotenv("key.env")) # load env vars

# api constants
DEFAULT_API_BASE = "https://openrouter.ai/api/v1"
DEFAULT_API_KEY = os.getenv("OPENROUTER_API_KEY", "")


class ProviderSortOptions(str, Enum):
    """Sort options for provider routing"""
    PRICE = "price"
    THROUGHPUT = "throughput"
    LATENCY = "latency"

class DataCollectionPolicy(str, Enum):
    """Provider data collection policy options"""
    ALLOW = "allow"
    DENY = "deny"

class ProviderPreferences(BaseModel):
    """Configuration for provider routing preferences"""
    order: Optional[List[str]] = None
    allow_fallbacks: Optional[bool] = None
    require_parameters: Optional[bool] = None
    data_collection: Optional[DataCollectionPolicy] = None
    ignore: Optional[List[str]] = None
    quantizations: Optional[List[str]] = None
    sort: Optional[ProviderSortOptions] = None

class ToolCall(BaseModel):
    """Representation of a tool call"""
    id: str
    type: str
    function: Dict[str, Any]

class FunctionCall(BaseModel):
    """Function call definition"""
    name: str
    arguments: str

class FunctionDescription(BaseModel):
    """Description of a function tool"""
    name: str
    description: Optional[str] = None
    parameters: Dict[str, Any]

class Tool(BaseModel):
    """Tool definition for model usage"""
    type: str = "function"
    function: FunctionDescription

class ReasoningConfig(BaseModel):
    """Configuration for reasoning tokens"""
    max_tokens: Optional[int] = None
    effort: Optional[Literal["high", "medium", "low"]] = None
    exclude: Optional[bool] = None

class ChatMessage(BaseModel):
    """Chat message format"""
    role: str
    content: Union[str, List[Dict[str, Any]]]
    name: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    tool_call_id: Optional[str] = None


class OpenRouterClient:
    """
    Client for interacting with OpenRouter's API.

    This class provides methods for listing models, making chat completions,
    and other operations supported by OpenRouter, using the OpenAI SDK format.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: str = DEFAULT_API_BASE,
        timeout: int = 60,
        app_name: Optional[str] = None,
    ):
        """
        Initialize the OpenRouter client.

        Args:
            api_key: OpenRouter API key [defaults to OPENROUTER_API_KEY env var]
            base_url: API base URL [defaults to OpenRouter API v1]
            timeout: Request timeout in seconds
            app_name: Optional app name to include in requests for tracking
        """
        self.api_key = api_key or DEFAULT_API_KEY
        self.base_url = base_url
        self.timeout = timeout
        self.app_name = app_name

        if not self.api_key:
            raise ValueError("OpenRouter API key is required. Set OPENROUTER_API_KEY environment variable or pass it to the constructor.")

        self.client = httpx.AsyncClient(
            timeout=timeout,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
        )

        # add app name header
        if not self.app_name:
            self.app_name = "Vizier"
        self.client.headers["X-Title"] = self.app_name


    async def list_models(
        self,
        filter_by: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        List available models from OpenRouter with optional filtering.

        Args:
            filter_by: Optional dictionary of filters to apply. Supported filters include:
                - provider: Filter by provider name (e.g., "openai", "anthropic")
                - max_price: Maximum price per token (e.g., 0.01)
                - min_context_length: Minimum context length (e.g., 8192)
                - feature: Specific feature support (e.g., "tool_calls", "vision")

        Returns:
            Dictionary containing model information.
        """
        url = f"{self.base_url}/models"

        # apply filters via query params if specified
        params = {}
        if filter_by:
            if "provider" in filter_by:
                params["provider"] = filter_by["provider"]
            if "max_price" in filter_by:
                params["max_price"] = filter_by["max_price"]
            if "min_context_length" in filter_by:
                params["min_context"] = filter_by["min_context_length"]
            if "feature" in filter_by:
                params["feature"] = filter_by["feature"]

        response = await self.client.get(url, params=params)

        if response.status_code != 200: # 200: OK
            self._handle_error_response(response)

        return response.json()


    async def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        stop: Optional[Union[str, List[str]]] = None,
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        seed: Optional[int] = None,
        provider: Optional[Dict[str, Any]] = None,
        transforms: Optional[List[str]] = None,
        reasoning: Optional[Dict[str, Any]] = None,
        max_price: Optional[Dict[str, float]] = None,
        plugins: Optional[List[Dict[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Create a chat completion using the specified model.

        Args:
            model: Model identifier (e.g., "openai/gpt-4o")
            messages: List of message dictionaries with role and content
            temperature: Sampling temperature (0-2)
            top_p: Nucleus sampling parameter (0-1)
            max_tokens: Maximum tokens to generate
            stream: Whether to stream the response
            tools: List of tools/functions the model can use
            tool_choice: Control for tool usage ("auto", "none" or specific tool)
            response_format: Format for the response (e.g., {"type": "json_object"})
            stop: Sequences where the API will stop generating
            frequency_penalty: Penalty for token frequency (-2 to 2)
            presence_penalty: Penalty for token presence (-2 to 2)
            seed: Random seed for deterministic outputs
            provider: Provider routing preferences
            transforms: Transformations to apply (e.g., ["middle-out"])
            reasoning: Configuration for reasoning tokens
            max_price: Maximum price constraints
            plugins: List of plugins to use (e.g., web search)

        Returns:
            Chat completion response
        """
        url = f"{self.base_url}/chat/completions"

        # build request payload
        payload = {
            "model": model,
            "messages": messages
        }

        # add optional params if specified
        if temperature is not None:
            payload["temperature"] = temperature
        if top_p is not None:
            payload["top_p"] = top_p
        if max_tokens is not None:
            payload["max_tokens"] = max_tokens
        if stream:
            payload["stream"] = stream
        if tools:
            payload["tools"] = tools
        if tool_choice:
            payload["tool_choice"] = tool_choice
        if response_format:
            payload["response_format"] = response_format
        if stop:
            payload["stop"] = stop
        if frequency_penalty is not None:
            payload["frequency_penalty"] = frequency_penalty
        if presence_penalty is not None:
            payload["presence_penalty"] = presence_penalty
        if seed is not None:
            payload["seed"] = seed
        if provider:
            payload["provider"] = provider
        if transforms:
            payload["transforms"] = transforms
        if reasoning:
            payload["reasoning"] = reasoning
        if max_price:
            payload["max_price"] = max_price
        if plugins:
            payload["plugins"] = plugins

        response = await self.client.post(url, json=payload)
        if response.status_code != 200: # 200: OK
            self._handle_error_response(response)

        return response.json()

    async def get_generation_details(self, generation_id: str) -> Dict[str, Any]:
        """
        Retrieve details about a specific generation, including tokens and cost.

        Args:
            generation_id: The generation ID to query

        Returns:
            Generation details including token counts, costs, and other metadata
        """
        url = f"{self.base_url}/generation"
        params = {"id": generation_id}

        response = await self.client.get(url, params=params)
        if response.status_code != 200: # 200: OK
            self._handle_error_response(response)

        return response.json()

    async def get_credits(self) -> Dict[str, Any]:
        """
        Get information about your OpenRouter credit balance.

        Returns:
            Dictionary containing credit information
        """
        url = f"{self.base_url}/credits"

        response = await self.client.get(url)
        if response.status_code != 200: # 200: OK
            self._handle_error_response(response)

        return response.json()

    def _handle_error_response(self, response: httpx.Response) -> None:
        """
        Handle error responses from the API.

        Args:
            response: The error response from the API

        Raises:
            Exception with error details
        """
        try:
            error_data = response.json()
            error_message = error_data.get("error", {}).get("message", "Unknown error")
            error_type = error_data.get("error", {}).get("type", "api_error")
            error_code = response.status_code

            raise Exception(f"OpenRouter API Error ({error_code}): {error_message} - {error_type}")
        except json.JSONDecodeError:
            raise Exception(f"OpenRouter API Error ({response.status_code}): {response.text}")




# create FastAPI router endpoints for OpenRouter integration
router = APIRouter(
    prefix="/openrouter",
    tags=["openrouter"],
    responses={404: {"description": "Not found"}},
)

# factory function to create OpenRouterClient instance
async def get_openrouter_client(
    api_key: Optional[str] = None,
    app_name: Optional[str] = None
) -> OpenRouterClient:
    """Create and return an OpenRouterClient instance."""
    return OpenRouterClient(api_key=api_key, app_name=app_name)

@router.get("/models")
async def list_models(
    provider: Optional[str] = None,
    max_price: Optional[float] = None,
    min_context_length: Optional[int] = None,
    feature: Optional[str] = None,
    client: OpenRouterClient = Depends(get_openrouter_client)
):
    """
    List available models with optional filters.

    Args:
        provider: Filter by provider name (e.g., "openai", "anthropic")
        max_price: Maximum price per token
        min_context_length: Minimum context length
        feature: Required feature support (e.g., "tool_calls", "vision")

    Returns:
        List of matching models
    """
    filters = {}
    if provider:
        filters["provider"] = provider
    if max_price:
        filters["max_price"] = max_price
    if min_context_length:
        filters["min_context_length"] = min_context_length
    if feature:
        filters["feature"] = feature

    try:
        models = await client.list_models(filter_by=filters)
        return models
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/chat/completions")
async def create_chat_completion(
    request: Dict[str, Any],
    client: OpenRouterClient = Depends(get_openrouter_client)
):
    """
    Create a chat completion using the specified parameters.

    The request body should include model, messages, and any optional parameters
    supported by OpenRouter's chat completion endpoint.

    Returns:
        Chat completion response
    """
    try:
        # extract required params
        model = request.pop("model")
        messages = request.pop("messages")

        # pass remaining params as kwargs
        response = await client.chat_completion(model=model, messages=messages, **request)
        return response

    except KeyError as e:
        raise HTTPException(status_code=400, detail=f"Missing required parameter: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/credits")
async def get_credits(
    client: OpenRouterClient = Depends(get_openrouter_client)
):
    """
    Get information about your OpenRouter credit balance.

    Returns:
        Credit information including balance
    """
    try:
        return await client.get_credits()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/generation/{generation_id}")
async def get_generation(
    generation_id: str,
    client: OpenRouterClient = Depends(get_openrouter_client)
):
    """
    Get details about a specific generation.

    Args:
        generation_id: ID of the generation to retrieve

    Returns:
        Generation details including token usage and costs
    """
    try:
        return await client.get_generation_details(generation_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

--------------------------------------------------
File End
--------------------------------------------------


.\routers\queries.py
File type: .py
from fastapi import APIRouter, Request, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
from database import database
from jose import jwt
import os, uuid, json, asyncio
from uuid import UUID 
import collections, re
from enum import Enum
from pydantic import BaseModel
from typing import Optional, Dict, Any, AsyncGenerator, List
from routers.openrouter import OpenRouterClient, get_openrouter_client
from datetime import datetime

router = APIRouter(prefix="/queries", tags=["queries"])

JWT_SECRET = os.getenv("JWT_SECRET")
if not JWT_SECRET:
    raise ValueError("JWT_SECRET environment variable must be set")

class ProcessStage(str, Enum):
    QUERY_RECEIVED = "query_received"
    REFINEMENT_STARTED = "refinement_started" 
    REFINEMENT_COMPLETED = "refinement_completed"
    ROUTING_STARTED = "routing_started"
    ROUTING_COMPLETED = "routing_completed"
    WEB_SEARCH_STARTED = "web_search_started"
    WEB_SEARCH_COMPLETED = "web_search_completed"
    TWITTER_SEARCH_STARTED = "twitter_search_started" 
    TWITTER_SEARCH_COMPLETED = "twitter_search_completed"
    SOURCE_RERANK_STARTED = "source_rerank_started"
    SOURCE_RERANK_COMPLETED = "source_rerank_completed"
    SOURCE_REVIEW_READY = "source_review_ready"
    SOURCE_REVIEW_COMPLETED = "source_review_completed"
    WRITING_STARTED = "writing_started"
    DRAFT_READY = "draft_ready"
    DRAFT_APPROVED = "draft_approved"
    COMPLETED = "completed"

class SessionState:
    def __init__(self, query_id: str):
        self.query_id = query_id
        self.stage = ProcessStage.QUERY_RECEIVED
        self.is_completed = False
        self._event_queue = asyncio.Queue()
        
    async def update_stage(self, new_stage: ProcessStage, data: Optional[Dict] = None):
        event = {
            "stage": new_stage,
            "timestamp": str(uuid.uuid1()),
            "data": data or {}
        }
        await self._event_queue.put(event)
        self.stage = new_stage
        
        if new_stage == ProcessStage.COMPLETED:
            self.is_completed = True
            
    async def event_generator(self) -> AsyncGenerator[Dict, None]:
        while not self.is_completed or not self._event_queue.empty():
            try:
                event = await self._event_queue.get()
                yield event
            except Exception as e:
                print(f"Error in event generator: {e}")
                break

# Global session states
active_sessions: Dict[str, SessionState] = {}

async def get_current_user(request: Request) -> str:
    """Get authenticated user ID from JWT token"""
    auth = request.headers.get("Authorization")
    if not auth or not auth.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Invalid authentication")
    
    try:
        token = auth.split(" ")[1]
        payload = jwt.decode(token, JWT_SECRET, algorithms=["HS256"])
        return payload.get("sub")
    except Exception as e:
        raise HTTPException(status_code=401, detail="Invalid token")

class SourceItem(BaseModel):
    """Source item with metadata"""
    url: str
    title: str
    content: str
    relevance_score: float
    source_type: str  # 'web' or 'twitter'
    timestamp: Optional[str]
    metadata: Dict[str, Any] = {}

class SourceReview(BaseModel):
    """User review of sources"""
    included: List[str]  # List of source URLs to include
    excluded: List[str]  # List of source URLs to exclude
    reranked_urls: List[str]  # Sources in desired order

@router.post("")
async def create_query(
    data: dict,
    request: Request,
    openrouter: OpenRouterClient = Depends(get_openrouter_client)
):
    """Create a new query"""
    user_id = await get_current_user(request)
    query_id = str(uuid.uuid4())
    
    try:
        await database.execute("""
            INSERT INTO queries (query_id, user_id, query_text, status)
            VALUES (:query_id, :user_id, :query_text, 'pending')
        """, {
            "query_id": query_id,
            "user_id": user_id,
            "query_text": data["query"]
        })
        
        # Initialize session state
        active_sessions[query_id] = SessionState(query_id)
        
        return {"query_id": query_id}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{query_id}")
async def get_query(query_id: str, request: Request):
    """Get query details"""
    user_id = await get_current_user(request)
    
    try:
        query = await database.fetch_one("""
            SELECT * FROM queries 
            WHERE query_id = :query_id AND user_id = :user_id
        """, {
            "query_id": query_id,
            "user_id": user_id
        })
        
        if not query:
            raise HTTPException(status_code=404, detail="Query not found")
            
        return dict(query)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/{query_id}/refine")
async def start_refinement(
    query_id: str, 
    data: dict,
    request: Request,
    openrouter: OpenRouterClient = Depends(get_openrouter_client)
):
    """Start query refinement process"""
    user_id = await get_current_user(request)
    
    if query_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Query session not found")
        
    session = active_sessions[query_id]
    await session.update_stage(ProcessStage.REFINEMENT_STARTED)
    
    try:
        # Get current query
        query = await database.fetch_one("""
            SELECT query_text FROM queries
            WHERE query_id = :query_id AND user_id = :user_id
        """, {
            "query_id": query_id,
            "user_id": user_id
        })
        
        if not query:
            raise HTTPException(status_code=404, detail="Query not found")
        
        current_query = query["query_text"]
        
        # Request refinement from LLM
        response = await openrouter.chat_completion(
            model="anthropic/claude-3-sonnet",
            messages=[{
                "role": "system",
                "content": "You are a research query refinement assistant. Help improve the clarity and specificity of research queries."
            }, {
                "role": "user", 
                "content": f"Please refine this research query: {current_query}"
            }]
        )
        
        refined_query = response["choices"][0]["message"]["content"]
        
        # Update database
        await database.execute("""
            UPDATE queries SET refined_query = :refined_query
            WHERE query_id = :query_id AND user_id = :user_id
        """, {
            "refined_query": refined_query,
            "query_id": query_id,
            "user_id": user_id
        })
        
        await session.update_stage(ProcessStage.REFINEMENT_COMPLETED)
        return {"refined_query": refined_query}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{query_id}/sources")
async def get_query_sources(query_id: str, request: Request):
    """Get sources for review"""
    user_id = await get_current_user(request)
    
    try:
        sources = await database.fetch_one("""
            SELECT web_sources, twitter_sources FROM queries
            WHERE query_id = :query_id AND user_id = :user_id
        """, {
            "query_id": query_id,
            "user_id": user_id
        })
        
        if not sources:
            raise HTTPException(status_code=404, detail="Query not found")
            
        return {
            "web_sources": sources["web_sources"],
            "twitter_sources": sources["twitter_sources"]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/{query_id}/sources/review")
async def submit_source_review(
    query_id: str,
    review: SourceReview,
    request: Request
):
    """Submit source review and reranking"""
    user_id = await get_current_user(request)
    
    if query_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Query session not found")
        
    session = active_sessions[query_id]
    
    try:
        # Get current sources
        sources = await database.fetch_one("""
            SELECT web_sources, twitter_sources FROM queries
            WHERE query_id = :query_id AND user_id = :user_id
        """, {
            "query_id": query_id,
            "user_id": user_id
        })
        
        if not sources:
            raise HTTPException(status_code=404, detail="Query not found")
            
        # Filter and rerank sources based on review
        web_sources = sources["web_sources"]
        twitter_sources = sources["twitter_sources"]
        
        # Filter out excluded sources
        filtered_web = [s for s in web_sources if s["url"] not in review.excluded]
        filtered_twitter = [s for s in twitter_sources if s["url"] not in review.excluded]
        
        # Rerank sources according to provided order
        reranked_sources = []
        for url in review.reranked_urls:
            source = next((s for s in filtered_web + filtered_twitter if s["url"] == url), None)
            if source:
                reranked_sources.append(source)
        
        # Update database with filtered and reranked sources
        await database.execute("""
            UPDATE queries 
            SET final_sources = :final_sources,
                web_sources = :web_sources,
                twitter_sources = :twitter_sources
            WHERE query_id = :query_id AND user_id = :user_id
        """, {
            "query_id": query_id,
            "user_id": user_id,
            "final_sources": json.dumps(reranked_sources),
            "web_sources": json.dumps(filtered_web),
            "twitter_sources": json.dumps(filtered_twitter)
        })
        
        await session.update_stage(ProcessStage.SOURCE_REVIEW_COMPLETED)
        return {"status": "success", "source_count": len(reranked_sources)}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stream/{query_id}")
async def stream_progress(query_id: str, request: Request):
    """Stream progress updates for a query process using SSE"""
    if query_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Query session not found")
    
    async def event_generator():
        session = active_sessions[query_id]
        try:
            async for event in session.event_generator():
                if await request.is_disconnected():
                    break
                yield {
                    "event": "message",
                    "data": json.dumps(event)
                }
        except Exception as e:
            print(f"Error in event stream: {e}")
        finally:
            if session.is_completed:
                active_sessions.pop(query_id, None)
    
    return EventSourceResponse(event_generator())

--------------------------------------------------
File End
--------------------------------------------------


.\routers\user.py
File type: .py
from fastapi import APIRouter, Request, Depends, HTTPException
from database import database
from jose import jwt
from uuid import UUID
import os

router = APIRouter(prefix="/user", tags=["user"])

JWT_SECRET = os.getenv("JWT_SECRET")

def get_current_user(request: Request):
    auth_header = request.headers.get("authorization")
    if not auth_header or not auth_header.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Authorization header missing or invalid")
    try:
        token = auth_header.split(" ")[1]
        payload = jwt.decode(token, JWT_SECRET, algorithms=["HS256"])
        return UUID(payload["sub"])
    except Exception as e:
        print("âŒ Invalid token:", e)
        raise HTTPException(status_code=401, detail="Invalid token")

@router.get("/me")
async def get_user_profile(request: Request):
    user_id = get_current_user(request)

    user = await database.fetch_one("""
        SELECT name, email, archetype, user_experience
        FROM users WHERE user_id = :user_id
    """, {"user_id": user_id})

    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    return {
        "name": user["name"],
        "email": user["email"],
        "archetype": user["archetype"],
        "user_experience": user["user_experience"]
    }

@router.post("/profile")
async def update_user_profile(data: dict, request: Request):
    user_id = get_current_user(request)

    update_query = """
    UPDATE users
    SET archetype = :archetype,
        user_description = :user_description,
        user_goals = :user_goals,
        user_experience = :user_experience
    WHERE user_id = :user_id
    """

    await database.execute(update_query, {
        "archetype": data.get("archetype"),
        "user_description": data.get("user_description"),
        "user_goals": data.get("user_goals"),
        "user_experience": data.get("user_experience"),
        "user_id": user_id
    })

    return {"message": "Profile updated", "data": data}


--------------------------------------------------
File End
--------------------------------------------------
